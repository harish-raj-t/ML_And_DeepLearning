{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f56267-e047-408b-b700-bfd5656dd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "path = r'C:\\\\Users\\\\harish-4072\\\\Downloads\\\\eng_french.csv'\n",
    "df = pd.read_csv(path, names=['English','French'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c2c616b-6aee-4c92-868f-cf4945194409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cf0886-675a-40c5-9619-a987c0a4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = text.split()  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1667dd2-4968-4760-8d63-c92c560667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = df['English'].dropna().apply(preprocess_text)\n",
    "english_vocab = Counter([token for sentence in english_sentences for token in sentence])\n",
    "\n",
    "french_sentences = df['French'].dropna().apply(preprocess_text)\n",
    "french_vocab = Counter([token for sentence in french_sentences for token in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc83af3-e8c3-4d21-bfa2-7fc287c5839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_to_id = {token: idx + 1 for idx, token in enumerate(english_vocab)}  # Start from 1 to reserve 0 for padding\n",
    "french_token_to_id = {token: idx + 3 for idx, token in enumerate(french_vocab)}\n",
    "\n",
    "english_token_to_id['<PAD>'] = 0\n",
    "\n",
    "french_token_to_id['<PAD>'] = 0\n",
    "french_token_to_id['<SOS>'] = 1\n",
    "french_token_to_id['<EOS>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6039b86c-6d89-4c69-99da-5ed3a7b7fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_id_to_token= {value:key for key,value in french_token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87b24936-8c74-4a24-a580-1554e1417daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_size = len(english_token_to_id)\n",
    "french_vocab_size = len(french_token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "388e6b58-a436-4f5b-a1a6-bcd61c2a405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens,token_to_id):\n",
    "    return [token_to_id.get(token,0) for token in tokens]\n",
    "\n",
    "english_sequences = english_sentences.apply(lambda x: tokenize_text(x, english_token_to_id))\n",
    "french_sequences = french_sentences.apply(lambda x: tokenize_text(x, french_token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc48382b-5a52-4c91-9a7d-5132007a48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sos_eos(tokens):\n",
    "    return [1]+tokens+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1aba5ed7-bf55-48d5-a86b-e4cc33d353ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sequences = french_sequences.apply(lambda x: add_sos_eos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "817ec228-6b92-49e7-9d32-b3b44ab5aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self,english_sequences,french_sequences):\n",
    "        self.english_sequences = english_sequences\n",
    "        self.french_sequences = french_sequences\n",
    "        assert len(self.english_sequences) == len(self.french_sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        X= self.english_sequences[idx]\n",
    "        y= self.french_sequences[idx]\n",
    "        return torch.tensor(X,dtype=torch.long),torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446f7892-79c8-44c8-8499-ca14d0ccfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    X,y = zip(*batch)\n",
    "    X_lengths = [len(item) for item in X]\n",
    "    y_lengths = [len(item) for item in y]\n",
    "    X_padded = pad_sequence(X, batch_first=False, padding_value=0)\n",
    "    y_padded = pad_sequence(y, batch_first=False, padding_value=0)\n",
    "    return X_padded, y_padded, X_lengths, y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa88e90-72cd-4224-b414-7a9f43cb3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_temp, french_temp = english_sequences[:50000].reset_index(drop=True), french_sequences[:50000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e400b7cc-0009-476c-8025-86442d5d5c47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_temp),len(french_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413527a1-4b0f-47b8-a2f0-a50cfd5d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentencesDataset(english_temp,french_temp)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_data_loader  = DataLoader(train_dataset, batch_size=256, shuffle=True,collate_fn = collate_fn)\n",
    "valid_data_loader  = DataLoader(val_dataset, batch_size=256, shuffle=False,collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3dc8b5d-ed00-4473-a5dc-37a8fa12172a",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(encoder_hidden_dim * 2, decoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
    "        # encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(\n",
    "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        )\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        return outputs, hidden\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d11a8a0-1e53-4bfb-8261-7dd7506295a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dim = english_vocab_size\n",
    "# output_dim = french_vocab_size\n",
    "# encoder_embedding_dim = 256\n",
    "# decoder_embedding_dim = 256\n",
    "# encoder_hidden_dim = 512\n",
    "# decoder_hidden_dim = 512\n",
    "# encoder_dropout = 0.5\n",
    "# decoder_dropout = 0.5\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    "\n",
    "# encoder = Encoder(\n",
    "#     input_dim,\n",
    "#     encoder_embedding_dim,\n",
    "#     encoder_hidden_dim,\n",
    "#     decoder_hidden_dim,\n",
    "#     encoder_dropout,\n",
    "# )\n",
    "# decoder = Decoder(\n",
    "#     output_dim,\n",
    "#     decoder_embedding_dim,\n",
    "#     encoder_hidden_dim,\n",
    "#     decoder_hidden_dim,\n",
    "#     decoder_dropout,\n",
    "#     attention,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a453c867-c8e0-491f-8589-075528278bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y,_,_ in train_data_loader:\n",
    "#     outputs, hidden = encoder(X)\n",
    "# print(outputs.shape, hidden.shape)\n",
    "# print(attention(hidden, outputs).shape)\n",
    "# input = torch.randint(0, 100, (1,32))\n",
    "# input.squeeze(0).shape\n",
    "# a,b,c = decoder(input.squeeze(0), hidden, outputs)\n",
    "# a.shape,b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed8f7413-6c05-41f6-8296-263fda451954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim, decoder_hidden_dim\n",
    "        )\n",
    "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_length = encoder_outputs.shape[0]\n",
    "        # repeat decoder hidden state src_length times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # hidden = [batch size, src length, decoder hidden dim]\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src length, decoder hidden dim]\n",
    "        attention = self.v_fc(energy).squeeze(2)\n",
    "        # attention = [batch size, src length]\n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6df9aa5-5813-4091-93e7-07db1db4af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        embedding_dim,\n",
    "        encoder_hidden_dim,\n",
    "        decoder_hidden_dim,\n",
    "        dropout,\n",
    "        attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU((encoder_hidden_dim * 2) + embedding_dim, decoder_hidden_dim)\n",
    "        self.fc_out = nn.Linear(\n",
    "            (encoder_hidden_dim * 2) + decoder_hidden_dim + embedding_dim, output_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, src length]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch size, 1, src length]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted = [batch size, 1, encoder hidden dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, encoder hidden dim * 2]\n",
    "       \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, (encoder hidden dim * 2) + embedding dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output = [seq length, batch size, decoder hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, decoder hid dim]\n",
    "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, decoder hidden dim]\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        # this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb7b9f3a-c3be-4b4e-92c4-0b8c41d01312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, decoder hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d221c5f3-a43a-4285-abf1-ceda45a2c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = english_vocab_size\n",
    "output_dim = french_vocab_size\n",
    "encoder_embedding_dim = 30\n",
    "decoder_embedding_dim = 30\n",
    "encoder_hidden_dim = 512\n",
    "decoder_hidden_dim = 512\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    decoder_dropout,\n",
    "    attention,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ef671ff-19bc-44c6-8a2b-25b711d6f03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(14393, 30)\n",
       "    (rnn): GRU(30, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn_fc): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v_fc): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(28062, 30)\n",
       "    (rnn): GRU(1054, 512)\n",
       "    (fc_out): Linear(in_features=1566, out_features=28062, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d8f0581-49c5-45a0-b79e-1fdd67c40718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,638,676 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c61fb32d-1915-4b24-86c4-8c854ce1ffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef6d37ac-79bb-4ca0-8f91-9d52cd9d4df1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for X, y, _, _ in data_loader:\n",
    "        src = X.to(device)\n",
    "        trg = y.to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30a9078b-391f-4286-94b3-ffbd4608218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y, _, _ in data_loader:\n",
    "            src = X.to(device)\n",
    "            trg = y.to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7afcf1c-b7e9-4890-9236-293220d66170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▉                                                                       | 1/10 [21:18<3:11:42, 1278.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   6.067 | Train PPL: 431.600\n",
      "\tValid Loss:   5.556 | Valid PPL: 258.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████                                                                | 2/10 [34:01<2:10:00, 975.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.172 | Train PPL: 176.188\n",
      "\tValid Loss:   5.065 | Valid PPL: 158.337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████                                                        | 3/10 [47:09<1:43:49, 889.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.535 | Train PPL:  93.250\n",
      "\tValid Loss:   4.523 | Valid PPL:  92.145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████████████████████▏                                              | 4/10 [1:00:02<1:24:22, 843.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.030 | Train PPL:  56.257\n",
      "\tValid Loss:   4.184 | Valid PPL:  65.597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████                                       | 5/10 [1:13:33<1:09:20, 832.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.618 | Train PPL:  37.279\n",
      "\tValid Loss:   3.927 | Valid PPL:  50.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████                                | 6/10 [1:27:21<55:22, 830.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   3.272 | Train PPL:  26.377\n",
      "\tValid Loss:   3.725 | Valid PPL:  41.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|████████████████████████████████████████████████                                | 6/10 [1:28:19<58:52, 883.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m best_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(n_epochs)):\n\u001b[1;32m----> 8\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate_fn(\n\u001b[0;32m     18\u001b[0m         model,\n\u001b[0;32m     19\u001b[0m         valid_data_loader,\n\u001b[0;32m     20\u001b[0m         criterion,\n\u001b[0;32m     21\u001b[0m         device,\n\u001b[0;32m     22\u001b[0m     )\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n",
      "Cell \u001b[1;32mIn[26], line 12\u001b[0m, in \u001b[0;36mtrain_fn\u001b[1;34m(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# src = [src length, batch size]\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# trg = [trg length, batch size]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# output = [trg length, batch size, trg vocab size]\u001b[39;00m\n\u001b[0;32m     14\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 36\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[0;32m     34\u001b[0m teacher_force \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m teacher_forcing_ratio\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# get the highest predicted token from our predictions\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m top1 \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# if teacher forcing, use actual next token as next input\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# if not, use predicted token\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m trg[t] \u001b[38;5;28;01mif\u001b[39;00m teacher_force \u001b[38;5;28;01melse\u001b[39;00m top1\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"tut3-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06ac63-1dfe-4b35-bfba-644770bb50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=25,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            de_tokens = [token.text for token in de_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            de_tokens = [token for token in sentence]\n",
    "        if lower:\n",
    "            de_tokens = [token.lower() for token in de_tokens]\n",
    "        de_tokens = [sos_token] + de_tokens + [eos_token]\n",
    "        ids = de_vocab.lookup_indices(de_tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        encoder_outputs, hidden = model.encoder(tensor)\n",
    "        inputs = en_vocab.lookup_indices([sos_token])\n",
    "        attentions = torch.zeros(max_output_length, 1, len(ids))\n",
    "        for i in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, attention = model.decoder(\n",
    "                inputs_tensor, hidden, encoder_outputs\n",
    "            )\n",
    "            attentions[i] = attention\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == en_vocab[eos_token]:\n",
    "                break\n",
    "        en_tokens = en_vocab.lookup_tokens(inputs)\n",
    "    return en_tokens, de_tokens, attentions[: len(en_tokens) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d0f-fb15-4502-94c7-2bd1914644c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(sentence, translation, attention):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    attention = attention.squeeze(1).numpy()\n",
    "    cax = ax.matshow(attention, cmap=\"bone\")\n",
    "    ax.set_xticks(ticks=np.arange(len(sentence)), labels=sentence, rotation=90, size=15)\n",
    "    translation = translation[1:]\n",
    "    ax.set_yticks(ticks=np.arange(len(translation)), labels=translation, size=15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd10880-48ca-499d-a106-a78b16d2353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_data[0][\"de\"]\n",
    "expected_translation = test_data[0][\"en\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb1a7f-acbb-4bb5-be09-d34c930b7c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_nlp,\n",
    "    de_nlp,\n",
    "    en_vocab,\n",
    "    de_vocab,\n",
    "    lower,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4061c-1245-4915-bd9b-f3a2ca65ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f4068-5fb6-4453-bf78-8814c311b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda201b3-f98c-4a99-af42-9286688ade6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(sentence_tokens, translation, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

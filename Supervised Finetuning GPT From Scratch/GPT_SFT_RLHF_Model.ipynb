{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x28dff850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import numpy as np\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_text_values(jsonl_file_path):\n",
    "    \"\"\"\n",
    "    Extract 'text' values from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_file_path (str): Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of extracted text values\n",
    "    \"\"\"\n",
    "    text_values = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as JSON\n",
    "                json_obj = json.loads(line.strip())\n",
    "                \n",
    "                # Extract the 'text' field if it exists\n",
    "                if 'text' in json_obj:\n",
    "                    text_values.append(json_obj['text'])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse line as JSON: {line}\")\n",
    "    \n",
    "    return text_values\n",
    "\n",
    "#file_path = \"/kaggle/input/gpt-dataset/gpt_dataset.jsonl\"\n",
    "file_path = \"D:\\ML_And_DeepLearning\\ML_And_DeepLearning\\Supervised Finetuning GPT From Scratch\\gpt_dataset.jsonl\"\n",
    "texts = extract_text_values(file_path)\n",
    "tokenized_texts = [word_tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Configurations | Desktop Management Configurations - ManageEngine Configurations You can use Desktop Central to complete different tasks \\n like scanning for patches and inventory. However, to complete tasks like \\n installing patches, changing the wallpaper of desktops in all the computers \\n in your network, sending custom messages to users in your network, and \\n installing software applications you must create configurations and deploy \\n them to the computers in your network. The following sections provide information required \\n to configure various Windows-application settings, security settings, \\n display settings, and firewall settings for Windows users and computers: Defining \\n    user configurations : This section provides information about various \\n    user-based configurations that you can deploy using Desktop Central and \\n    the steps to define them. Defining \\n    computer configurations : This section provides information about various \\n    computer-based configurations that you can deploy using Desktop Central \\n    and the steps to define them. Configuring \\n    collections : This section provides information about defining a collection \\n    of configurations and steps required to deploy them simultaneously to \\n    several users or computers. Defining \\n    targets : This section provides information about defining targets \\n    to which you want to deploy configurations or collections. Configuring \\n    execution settings : This section provides information about configuring \\n    execution settings while defining a configuration. Desktop Central enables \\n    you to automate the redeployment process through the Execution Settings \\n    option. Managing \\n    configurations and collections : This section provides information \\n    about managing defined configurations. It gives you information about \\n    the following: Various \\n        configuration statuses displayed on the Desktop Central server Modifying \\n        configurations or collections Viewing \\n        the status of the defined configurations or collections Suspending \\n        deployment Resuming \\n        suspended deployments Viewing \\n\\t  configuration reports : This section provides information about viewing \\n\\t  a detailed report about configurations that you define and deploy using \\n\\t  Desktop Central. You can also view the status of each configuration in \\n  this report. Viewing \\n    system-uptime reports : This section provides the details of uptime \\n    and downtime of computers during a specific period. Top'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter([token for sentence in tokenized_texts for token in sentence])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)} \n",
    "id_to_token= {value:key for key,value in token_to_id.items()}\n",
    "vocab_size = len(id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens):\n",
    "    return [token_to_id.get(token,0) for token in tokens]\n",
    "\n",
    "dataset = [tokenize_text(text) for text in tokenized_texts if len(text) > BLOCK_SIZE+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(dataset, block_size):\n",
    "    lengths = [len(datapoint)-block_size for datapoint in dataset]\n",
    "    cumulative_lengths = [0]\n",
    "    prev_length = 0  \n",
    "    for length in lengths:\n",
    "        temp = length + prev_length\n",
    "        cumulative_lengths.append(temp)\n",
    "        prev_length = temp\n",
    "    total_len = cumulative_lengths[-1]\n",
    "    current_datapoint = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx in range(total_len):\n",
    "        if idx >= cumulative_lengths[current_datapoint+1]:\n",
    "            current_datapoint +=1\n",
    "        datapoint_idx = (idx - cumulative_lengths[current_datapoint])\n",
    "          \n",
    "        X.append(dataset[current_datapoint][datapoint_idx:datapoint_idx+block_size])\n",
    "        y.append(dataset[current_datapoint][datapoint_idx+1:datapoint_idx+block_size+1])\n",
    "    return X,y\n",
    "X,y = construct_dataset(dataset, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    return torch.tensor(X[idx],dtype=torch.long).to(device),torch.tensor(y[idx],dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x0000000054137100> <torch.utils.data.dataset.Subset object at 0x0000000052FE9A20>\n"
     ]
    }
   ],
   "source": [
    "data = CustomDataset(dataset,BLOCK_SIZE)\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "print(train_dataset, val_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "  def __init__(self, emd_dim, heads=4, dropout = 0.2):\n",
    "    super(MaskedMultiHeadAttention, self).__init__()\n",
    "    assert emd_dim % heads == 0\n",
    "    self.heads = heads\n",
    "    self.head_dim = emd_dim//heads\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "    self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n",
    "    self.output = nn.Linear(emd_dim,emd_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    qkv = self.multiHead(x)\n",
    "    q, k, v = torch.chunk(qkv,3,dim=-1)\n",
    "    q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "    tril = torch.tril(torch.ones(T,T).to(device))\n",
    "    attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    attn_probs_drop = self.dropout(attn_probs)\n",
    "    attn_output = torch.matmul(attn_probs_drop,v)\n",
    "    fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n",
    "    return self.output(fn_attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1D(nn.Module):\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    super(LayerNorm1D, self).__init__()\n",
    "    self.gamma = nn.Parameter(torch.ones(dim).to(device))\n",
    "    self.beta = nn.Parameter(torch.zeros(dim).to(device))\n",
    "    self.eps = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(-1,keepdim=True)\n",
    "    var = x.var(-1, unbiased=False, keepdim=True)\n",
    "    xhat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "    return (self.gamma * xhat) +self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n",
    "    super().__init__()\n",
    "    self.feed_forward_layer = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, output_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.feed_forward_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self,embed_dim,heads=4):\n",
    "    super().__init__()\n",
    "    self.layer_norm1 = LayerNorm1D(embed_dim)\n",
    "    self.layer_norm2 = LayerNorm1D(embed_dim)\n",
    "    self.masked_multi_head_attn =  MaskedMultiHeadAttention(embed_dim, heads = 4)\n",
    "    self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.masked_multi_head_attn(self.layer_norm1(x))\n",
    "    x = x + self.feed_forward_layer(self.layer_norm2(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveModel(nn.Module):\n",
    "  def __init__(self, embed_dim, vocab_size, block_size = BLOCK_SIZE, heads=4, num_layers=4):\n",
    "    super().__init__()\n",
    "    self.block = nn.Sequential(*[Block(embed_dim,heads) for _ in range(num_layers)])\n",
    "    self.positional_embedding = nn.Embedding(block_size, embed_dim)\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.final_layer_norm = LayerNorm1D(embed_dim)\n",
    "    self.final_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, T = x.shape\n",
    "    x_emb = self.embedding(x)\n",
    "    x_pos_emb = self.positional_embedding(torch.arange(T).to(device))\n",
    "    x = x_emb + x_pos_emb\n",
    "    block_output = self.block(x)\n",
    "    x_out = self.final_layer_norm(block_output)\n",
    "    return self.final_layer(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harish-4072\\AppData\\Local\\Temp\\ipykernel_51316\\1775489623.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"gpt_sft_rlhf.pth\", map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for AutoRegressiveModel:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([6327, 128]) from checkpoint, the shape in current model is torch.Size([6292, 128]).\n\tsize mismatch for final_layer.weight: copying a param with shape torch.Size([6327, 128]) from checkpoint, the shape in current model is torch.Size([6292, 128]).\n\tsize mismatch for final_layer.bias: copying a param with shape torch.Size([6327]) from checkpoint, the shape in current model is torch.Size([6292]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoRegressiveModel(embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, block_size\u001b[38;5;241m=\u001b[39m BLOCK_SIZE, heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt_sft_rlhf.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt_sft_rlhf.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AutoRegressiveModel:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([6327, 128]) from checkpoint, the shape in current model is torch.Size([6292, 128]).\n\tsize mismatch for final_layer.weight: copying a param with shape torch.Size([6327, 128]) from checkpoint, the shape in current model is torch.Size([6292, 128]).\n\tsize mismatch for final_layer.bias: copying a param with shape torch.Size([6327]) from checkpoint, the shape in current model is torch.Size([6292])."
     ]
    }
   ],
   "source": [
    "model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size= BLOCK_SIZE, heads = 4).to(device)\n",
    "if os.path.exists(\"gpt_sft_rlhf.pth\"):\n",
    "    model.load_state_dict(torch.load(\"gpt_sft_rlhf.pth\", map_location=torch.device('cpu'))) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim, criterion: nn.Module, dataloader: DataLoader, epochs: int):\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X,y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      print(X.shape)\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "    torch.save(model.state_dict(), \"gpt_sft_rlhf.pth\")\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model: nn.Module,dataloader: DataLoader):\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for X,y in dataloader:\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      val_loss += loss.item()\n",
    "    print(f\"Loss: {val_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, optimizer, criterion, train_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: nn.Module, start_seq: str =\"ManageEngine helps in \", epochs = 100):\n",
    "  tokenized_test = [word_tokenize(start_seq)]\n",
    "  current = torch.tensor([tokenize_text(text) for text in tokenized_test])\n",
    "  content = start_seq\n",
    "  for _ in range(epochs):\n",
    "    if current.size(1) > BLOCK_SIZE:\n",
    "        current = current[:, -BLOCK_SIZE:]\n",
    "    outputs = model(current)\n",
    "    probs = torch.softmax(outputs[:,-1,:].squeeze(0), dim=-1)\n",
    "    indices = torch.multinomial(probs,1).tolist()\n",
    "    output = id_to_token[indices[0]]\n",
    "    content = content + \" \"+ output\n",
    "    current = torch.cat((current,torch.tensor(indices[0]).unsqueeze(0).unsqueeze(0)), dim = -1)\n",
    "    print(content)  \n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_tokens, max_new_tokens=100, temperature=1.0, top_k=None):\n",
    "    \n",
    "    model.eval()\n",
    "    x = torch.tensor([start_tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    max_tokens = min(max_new_tokens, BLOCK_SIZE - len(start_tokens))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            if x.size(1) > BLOCK_SIZE:\n",
    "                x = x[:, -BLOCK_SIZE:]\n",
    "                \n",
    "            logits = model(x)\n",
    "            \n",
    "            # Get logits for the next token (last position)\n",
    "            next_token_logits = logits[0, -1, :]\n",
    "            \n",
    "            # Apply temperature scaling\n",
    "            next_token_logits = next_token_logits / max(temperature, 1e-8)\n",
    "            \n",
    "            # Apply top-k filtering if specified\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                next_token_logits[next_token_logits < v[-1]] = float('-inf')\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            # Append to the sequence\n",
    "            x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    return x[0].tolist()\n",
    "\n",
    "def tokens_to_text(token_ids):\n",
    "    \"\"\"\n",
    "    Convert a list of token ids back to readable text.\n",
    "    \n",
    "    Args:\n",
    "        token_ids: List of token ids\n",
    "        \n",
    "    Returns:\n",
    "        String of reconstructed text\n",
    "    \"\"\"\n",
    "    # Convert token ids to tokens\n",
    "    tokens = [id_to_token.get(id, \"<UNK>\") for id in token_ids]\n",
    "    \n",
    "    # Join tokens with spaces (this is a simplified approach)\n",
    "    # For a more sophisticated approach, you'd need to handle punctuation\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    # Basic cleanup\n",
    "    # Replace multiple spaces with single space\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Handle common punctuation\n",
    "    for punct in ['.', ',', '!', '?', ':', ';']:\n",
    "        text = text.replace(f\" {punct}\", punct)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example usage\n",
    "def run_inference_example():\n",
    "    # Load the model if not already loaded\n",
    "    inference_model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size=BLOCK_SIZE, heads=4).to(device)\n",
    "    inference_model.load_state_dict(torch.load(\"gpt_sft_rlhf.pth\"))\n",
    "    \n",
    "    # Example starting prompt\n",
    "    prompt = \"The quick brown fox\"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    prompt_tokens = word_tokenize(prompt)\n",
    "    prompt_ids = tokenize_text(prompt_tokens)\n",
    "    \n",
    "    # Generate text\n",
    "    print(f\"Generating with prompt: '{prompt}'\")\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    # Try different generation parameters\n",
    "    for temp in [0.7, 1.0]:\n",
    "        for top_k in [None, 10]:\n",
    "            print(f\"\\nTemperature: {temp}, Top-k: {top_k}\")\n",
    "            generated_ids = generate_text(\n",
    "                inference_model, \n",
    "                prompt_ids, \n",
    "                max_new_tokens=50, \n",
    "                temperature=temp,\n",
    "                top_k=top_k\n",
    "            )\n",
    "            \n",
    "            generated_text = tokens_to_text(generated_ids)\n",
    "            print(generated_text)\n",
    "    \n",
    "    return inference_model\n",
    "\n",
    "# Interactive text generation\n",
    "def interactive_generation(model):\n",
    "    print(\"\\nInteractive Text Generation\")\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        prompt = input(\"\\nEnter a prompt: \")\n",
    "        if prompt.lower() == 'exit':\n",
    "            break\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        prompt_tokens = word_tokenize(prompt)\n",
    "        prompt_ids = tokenize_text(prompt_tokens)\n",
    "        \n",
    "        if not prompt_ids:\n",
    "            print(\"Error: Could not tokenize prompt. Try again.\")\n",
    "            continue\n",
    "        \n",
    "        # Get generation parameters\n",
    "        try:\n",
    "            temp = float(input(\"Temperature (0.1-2.0, default 1.0): \") or 1.0)\n",
    "            max_tokens = int(input(\"Max tokens to generate (default 50): \") or 50)\n",
    "            top_k_input = input(\"Top-k (integer, or leave empty for no top-k): \")\n",
    "            top_k = int(top_k_input) if top_k_input.strip() else None\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, using defaults.\")\n",
    "            temp = 1.0\n",
    "            max_tokens = 50\n",
    "            top_k = None\n",
    "        \n",
    "        # Generate text\n",
    "        generated_ids = generate_text(\n",
    "            model, \n",
    "            prompt_ids, \n",
    "            max_new_tokens=max_tokens, \n",
    "            temperature=temp,\n",
    "            top_k=top_k\n",
    "        )\n",
    "        \n",
    "        generated_text = tokens_to_text(generated_ids)\n",
    "        print(\"\\nGenerated text:\")\n",
    "        print(generated_text)\n",
    "\n",
    "# Run the inference example\n",
    "if __name__ == \"__main__\":\n",
    "    model = run_inference_example()\n",
    "    interactive_generation(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6826867,
     "sourceId": 10971507,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02da6ed-821a-4454-be5f-838432404f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2bf00b10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import numpy as np\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5147f00f-9e6e-495b-8331-fc8245e8b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_text_values(jsonl_file_path):\n",
    "    \"\"\"\n",
    "    Extract 'text' values from a JSONL file.\n",
    "    \n",
    "    Args:\n",
    "        jsonl_file_path (str): Path to the JSONL file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of extracted text values\n",
    "    \"\"\"\n",
    "    text_values = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as JSON\n",
    "                json_obj = json.loads(line.strip())\n",
    "                \n",
    "                # Extract the 'text' field if it exists\n",
    "                if 'text' in json_obj:\n",
    "                    text_values.append(json_obj['text'])\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Warning: Could not parse line as JSON: {line}\")\n",
    "    \n",
    "    return text_values\n",
    "\n",
    "file_path = \"gpt_dataset.jsonl\"\n",
    "texts = extract_text_values(file_path)\n",
    "tokenized_texts = [word_tokenize(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8d8399-fa26-4c85-947e-c27254463b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter([token for sentence in tokenized_texts for token in sentence])\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)} \n",
    "id_to_token= {value:key for key,value in token_to_id.items()}\n",
    "vocab_size = len(id_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec46413e-aac2-4a46-a932-c8847808d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens):\n",
    "    return [token_to_id.get(token,0) for token in tokens]\n",
    "\n",
    "dataset = [tokenize_text(text) for text in tokenized_texts if len(text) > 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e3049c-ceea-430c-bbb9-851520a99500",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd115d25-e627-41a5-b8e8-c994935b9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataset(dataset, block_size):\n",
    "    lengths = [len(datapoint)-block_size for datapoint in dataset]\n",
    "    cumulative_lengths = [0]\n",
    "    prev_length = 0  \n",
    "    for length in lengths:\n",
    "        temp = length + prev_length\n",
    "        cumulative_lengths.append(temp)\n",
    "        prev_length = temp\n",
    "    total_len = cumulative_lengths[-1]\n",
    "    current_datapoint = 0\n",
    "    X = []\n",
    "    y = []\n",
    "    for idx in range(total_len):\n",
    "        if idx >= cumulative_lengths[current_datapoint+1]:\n",
    "            current_datapoint +=1\n",
    "        datapoint_idx = (idx - cumulative_lengths[current_datapoint])\n",
    "          \n",
    "        X.append(dataset[current_datapoint][datapoint_idx:datapoint_idx+block_size])\n",
    "        y.append(dataset[current_datapoint][datapoint_idx+1:datapoint_idx+block_size+1])\n",
    "    return X,y\n",
    "X,y = construct_dataset(dataset, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39e6587c-4b92-415f-9d55-cb6f9663841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "  def __init__(self,X, y):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    return torch.tensor(X[idx],dtype=torch.long).to(device),torch.tensor(y[idx],dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab879705-ae01-448a-88cf-3472b060ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0555cda-4389-454b-a14e-e50478fc2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x00000000551883D0> <torch.utils.data.dataset.Subset object at 0x0000000055189CC0>\n"
     ]
    }
   ],
   "source": [
    "data = CustomDataset(dataset,BLOCK_SIZE)\n",
    "train_size = int(0.8 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "train_dataset, val_dataset = random_split(data, [train_size, val_size])\n",
    "print(train_dataset, val_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f9a1c87-4485-4f84-837f-1c7af390c231",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "  def __init__(self, emd_dim, heads=4, dropout = 0.2):\n",
    "    super(MaskedMultiHeadAttention, self).__init__()\n",
    "    assert emd_dim % heads == 0\n",
    "    self.heads = heads\n",
    "    self.head_dim = emd_dim//heads\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "    self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n",
    "    self.output = nn.Linear(emd_dim,emd_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    qkv = self.multiHead(x)\n",
    "    q, k, v = torch.chunk(qkv,3,dim=-1)\n",
    "    q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    attn_probs_drop = self.dropout(attn_probs)\n",
    "    attn_output = torch.matmul(attn_probs_drop,v)\n",
    "    fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n",
    "    return self.output(fn_attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7631cce1-701a-4dfe-9325-8ab1c57efcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1D(nn.Module):\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    super(LayerNorm1D, self).__init__()\n",
    "    self.gamma = nn.Parameter(torch.ones(dim))\n",
    "    self.beta = nn.Parameter(torch.zeros(dim))\n",
    "    self.eps = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(-1,keepdim=True)\n",
    "    var = x.var(-1, unbiased=False, keepdim=True)\n",
    "    xhat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "    return (self.gamma * xhat) +self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d59eca-8e7f-4648-80ee-2128a65489eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n",
    "    super().__init__()\n",
    "    self.feed_forward_layer = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, output_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.feed_forward_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "015353c2-3b47-4f96-a6eb-771cc77955f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self,embed_dim,heads=4):\n",
    "    super().__init__()\n",
    "    self.layer_norm1 = LayerNorm1D(embed_dim)\n",
    "    self.layer_norm2 = LayerNorm1D(embed_dim)\n",
    "    self.masked_multi_head_attn =  MaskedMultiHeadAttention(embed_dim, heads = 4)\n",
    "    self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.masked_multi_head_attn(self.layer_norm1(x))\n",
    "    x = x + self.feed_forward_layer(self.layer_norm2(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65b54056-2f4a-4439-88d4-59ae61135a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveModel(nn.Module):\n",
    "  def __init__(self, embed_dim, vocab_size, block_size = BLOCK_SIZE, heads=4, num_layers=4):\n",
    "    super().__init__()\n",
    "    self.block = nn.Sequential(*[Block(embed_dim,heads) for _ in range(num_layers)])\n",
    "    self.positional_embedding = nn.Embedding(block_size, embed_dim)\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.final_layer_norm = LayerNorm1D(embed_dim)\n",
    "    self.final_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    _, T = x.shape\n",
    "    x_emb = self.embedding(x)\n",
    "    x_pos_emb = self.positional_embedding(torch.arange(T))\n",
    "    x = x_emb + x_pos_emb\n",
    "    block_output = self.block(x)\n",
    "    x_out = self.final_layer_norm(block_output)\n",
    "    return self.final_layer(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0e88ad8-2863-4857-8f1e-28e77934cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size= BLOCK_SIZE, heads = 4).to(device)\n",
    "if os.path.exists(\"gpt_sft_rlhf.pth\"):\n",
    "    model.load_state_dict(torch.load(\"gpt_sft_rlhf.pth\")) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d850271-511e-4893-9365-24ee14bac5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim, criterion: nn.Module, dataloader: DataLoader, epochs: int):\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X,y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "      print(X.shape)\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38e4643d-acf4-4d8d-8f90-a7bede88ac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model: nn.Module,dataloader: DataLoader):\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for X,y in dataloader:\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      val_loss += loss.item()\n",
    "    print(f\"Loss: {val_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2176af2-9b8d-402c-afd0-0e35a20f36db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "Epoch: 1/10, Loss: 9.0764\n",
      "torch.Size([8, 8])\n",
      "Epoch: 2/10, Loss: 7.8580\n",
      "torch.Size([8, 8])\n",
      "Epoch: 3/10, Loss: 6.9489\n",
      "torch.Size([8, 8])\n",
      "Epoch: 4/10, Loss: 6.2954\n",
      "torch.Size([8, 8])\n",
      "Epoch: 5/10, Loss: 5.7897\n",
      "torch.Size([8, 8])\n",
      "Epoch: 6/10, Loss: 5.2842\n",
      "torch.Size([8, 8])\n",
      "Epoch: 7/10, Loss: 4.8422\n",
      "torch.Size([8, 8])\n",
      "Epoch: 8/10, Loss: 4.3657\n",
      "torch.Size([8, 8])\n",
      "Epoch: 9/10, Loss: 3.9392\n",
      "torch.Size([8, 8])\n",
      "Epoch: 10/10, Loss: 3.5539\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2627de1-62a3-4d6c-8bea-ae3c571926de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val(model,val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10971507,"sourceType":"datasetVersion","datasetId":6826867}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom collections import Counter\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport os\nimport numpy as np\nseed = 1234\ntorch.manual_seed(seed)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\ndef extract_text_values(jsonl_file_path):\n    \"\"\"\n    Extract 'text' values from a JSONL file.\n    \n    Args:\n        jsonl_file_path (str): Path to the JSONL file\n        \n    Returns:\n        list: List of extracted text values\n    \"\"\"\n    text_values = []\n    \n    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n        for line in file:\n            try:\n                # Parse each line as JSON\n                json_obj = json.loads(line.strip())\n                \n                # Extract the 'text' field if it exists\n                if 'text' in json_obj:\n                    text_values.append(json_obj['text'])\n            except json.JSONDecodeError:\n                print(f\"Warning: Could not parse line as JSON: {line}\")\n    \n    return text_values\n\nfile_path = \"/kaggle/input/gpt-dataset/gpt_dataset.jsonl\"\ntexts = extract_text_values(file_path)\ntokenized_texts = [word_tokenize(text) for text in texts]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vocab = Counter([token for sentence in tokenized_texts for token in sentence])\ntoken_to_id = {token: idx for idx, token in enumerate(vocab)} \nid_to_token= {value:key for key,value in token_to_id.items()}\nvocab_size = len(id_to_token)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_text(tokens):\n    return [token_to_id.get(token,0) for token in tokens]\n\ndataset = [tokenize_text(text) for text in tokenized_texts if len(text) > 9]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#dataset = dataset[:10]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def construct_dataset(dataset, block_size):\n    lengths = [len(datapoint)-block_size for datapoint in dataset]\n    cumulative_lengths = [0]\n    prev_length = 0  \n    for length in lengths:\n        temp = length + prev_length\n        cumulative_lengths.append(temp)\n        prev_length = temp\n    total_len = cumulative_lengths[-1]\n    current_datapoint = 0\n    X = []\n    y = []\n    for idx in range(total_len):\n        if idx >= cumulative_lengths[current_datapoint+1]:\n            current_datapoint +=1\n        datapoint_idx = (idx - cumulative_lengths[current_datapoint])\n          \n        X.append(dataset[current_datapoint][datapoint_idx:datapoint_idx+block_size])\n        y.append(dataset[current_datapoint][datapoint_idx+1:datapoint_idx+block_size+1])\n    return X,y\nX,y = construct_dataset(dataset, 8)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass CustomDataset(Dataset):\n  def __init__(self,X, y):\n    self.X = X\n    self.y = y\n\n  def __len__(self):\n    return len(self.X)\n\n  def __getitem__(self,idx):\n    return torch.tensor(X[idx],dtype=torch.long).to(device),torch.tensor(y[idx],dtype=torch.long).to(device)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"BLOCK_SIZE = 8","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = CustomDataset(dataset,BLOCK_SIZE)\ntrain_size = int(0.8 * len(data))\nval_size = len(data) - train_size\ntrain_dataset, val_dataset = random_split(data, [train_size, val_size])\nprint(train_dataset, val_dataset)\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaskedMultiHeadAttention(nn.Module):\n  def __init__(self, emd_dim, heads=4, dropout = 0.2):\n    super(MaskedMultiHeadAttention, self).__init__()\n    assert emd_dim % heads == 0\n    self.heads = heads\n    self.head_dim = emd_dim//heads\n    self.scale = self.head_dim ** -0.5\n    self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n    self.output = nn.Linear(emd_dim,emd_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B, T, C = x.shape\n    qkv = self.multiHead(x)\n    q, k, v = torch.chunk(qkv,3,dim=-1)\n    q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n    tril = torch.tril(torch.ones(T,T).to(device))\n    attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n    attn_probs = torch.softmax(attn_scores, dim=-1)\n    attn_probs_drop = self.dropout(attn_probs)\n    attn_output = torch.matmul(attn_probs_drop,v)\n    fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n    return self.output(fn_attn_output)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LayerNorm1D(nn.Module):\n  def __init__(self, dim, eps=1e-5):\n    super(LayerNorm1D, self).__init__()\n    self.gamma = nn.Parameter(torch.ones(dim).to(device))\n    self.beta = nn.Parameter(torch.zeros(dim).to(device))\n    self.eps = eps\n\n  def forward(self, x):\n    mean = x.mean(-1,keepdim=True)\n    var = x.var(-1, unbiased=False, keepdim=True)\n    xhat = (x-mean)/torch.sqrt(var+self.eps)\n    return (self.gamma * xhat) +self.beta","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FeedForward(nn.Module):\n  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n    super().__init__()\n    self.feed_forward_layer = nn.Sequential(\n      nn.Linear(input_dim, hidden_dim),\n      nn.ReLU(),\n      nn.Linear(hidden_dim, output_dim),\n      nn.Dropout(dropout)\n    )\n\n  def forward(self, x):\n    return self.feed_forward_layer(x)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Block(nn.Module):\n  def __init__(self,embed_dim,heads=4):\n    super().__init__()\n    self.layer_norm1 = LayerNorm1D(embed_dim)\n    self.layer_norm2 = LayerNorm1D(embed_dim)\n    self.masked_multi_head_attn =  MaskedMultiHeadAttention(embed_dim, heads = 4)\n    self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n\n  def forward(self, x):\n    x = x + self.masked_multi_head_attn(self.layer_norm1(x))\n    x = x + self.feed_forward_layer(self.layer_norm2(x))\n    return x\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AutoRegressiveModel(nn.Module):\n  def __init__(self, embed_dim, vocab_size, block_size = BLOCK_SIZE, heads=4, num_layers=4):\n    super().__init__()\n    self.block = nn.Sequential(*[Block(embed_dim,heads) for _ in range(num_layers)])\n    self.positional_embedding = nn.Embedding(block_size, embed_dim)\n    self.embedding = nn.Embedding(vocab_size, embed_dim)\n    self.final_layer_norm = LayerNorm1D(embed_dim)\n    self.final_layer = nn.Linear(embed_dim, vocab_size)\n\n  def forward(self, x):\n    _, T = x.shape\n    x_emb = self.embedding(x)\n    x_pos_emb = self.positional_embedding(torch.arange(T).to(device))\n    x = x_emb + x_pos_emb\n    block_output = self.block(x)\n    x_out = self.final_layer_norm(block_output)\n    return self.final_layer(x_out)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size= BLOCK_SIZE, heads = 4).to(device)\nif os.path.exists(\"gpt_sft_rlhf.pth\"):\n    model.load_state_dict(torch.load(\"gpt_sft_rlhf.pth\")) \noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\ncriterion = nn.CrossEntropyLoss()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model: nn.Module, optimizer: torch.optim, criterion: nn.Module, dataloader: DataLoader, epochs: int):\n\n  for epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for X,y in dataloader:\n      optimizer.zero_grad()\n      print(X.shape)\n      outputs = model(X)\n      B, T, _ = outputs.shape\n      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n      loss.backward()\n      optimizer.step()\n      epoch_loss += loss.item()\n    torch.save(model.state_dict(), \"gpt_sft_rlhf.pth\")\n    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val(model: nn.Module,dataloader: DataLoader):\n  model.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for X,y in dataloader:\n      outputs = model(X)\n      B, T, _ = outputs.shape\n      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n      val_loss += loss.item()\n    print(f\"Loss: {val_loss / len(dataloader):.4f}\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(model, optimizer, criterion, train_loader, 20)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val(model,val_loader)","metadata":{},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":859181,"sourceType":"datasetVersion","datasetId":455483},{"sourceId":291118,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":249440,"modelId":270959},{"sourceId":293268,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":251197,"modelId":272676}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:46.489948Z","iopub.execute_input":"2025-03-20T11:27:46.490431Z","iopub.status.idle":"2025-03-20T11:27:46.510211Z","shell.execute_reply.started":"2025-03-20T11:27:46.490380Z","shell.execute_reply":"2025-03-20T11:27:46.508886Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"!pip install open-tamil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:46.511792Z","iopub.execute_input":"2025-03-20T11:27:46.512179Z","iopub.status.idle":"2025-03-20T11:27:51.413755Z","shell.execute_reply.started":"2025-03-20T11:27:46.512146Z","shell.execute_reply":"2025-03-20T11:27:51.412104Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: open-tamil in /usr/local/lib/python3.10/dist-packages (1.1)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport tamil\nimport sentencepiece as spm\nimport os\nseed = 1234\ntorch.manual_seed(seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.416121Z","iopub.execute_input":"2025-03-20T11:27:51.416448Z","iopub.status.idle":"2025-03-20T11:27:51.428138Z","shell.execute_reply.started":"2025-03-20T11:27:51.416420Z","shell.execute_reply":"2025-03-20T11:27:51.426646Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7dc07e3f9eb0>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"BLOCK_SIZE = 16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.429794Z","iopub.execute_input":"2025-03-20T11:27:51.430136Z","iopub.status.idle":"2025-03-20T11:27:51.445056Z","shell.execute_reply.started":"2025-03-20T11:27:51.430107Z","shell.execute_reply":"2025-03-20T11:27:51.443414Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class MaskedMultiHeadAttention(nn.Module):\n  def __init__(self, emd_dim, heads=4, dropout = 0.2):\n    super(MaskedMultiHeadAttention, self).__init__()\n    assert emd_dim % heads == 0\n    self.heads = heads\n    self.head_dim = emd_dim//heads\n    self.scale = self.head_dim ** -0.5\n    self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n    self.output = nn.Linear(emd_dim,emd_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B, T, C = x.shape\n    qkv = self.multiHead(x)\n    q, k, v = torch.chunk(qkv,3,dim=-1)\n    q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n    tril = torch.tril(torch.ones(T,T)).to(device)\n    attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n    attn_probs = torch.softmax(attn_scores, dim=-1)\n    attn_probs_drop = self.dropout(attn_probs)\n    attn_output = torch.matmul(attn_probs_drop,v)\n    fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n    return self.output(fn_attn_output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.447113Z","iopub.execute_input":"2025-03-20T11:27:51.447584Z","iopub.status.idle":"2025-03-20T11:27:51.464113Z","shell.execute_reply.started":"2025-03-20T11:27:51.447532Z","shell.execute_reply":"2025-03-20T11:27:51.462969Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class LayerNorm1D(nn.Module):\n  def __init__(self, dim, eps=1e-5):\n    super(LayerNorm1D, self).__init__()\n    self.gamma = nn.Parameter(torch.ones(dim)).to(device)\n    self.beta = nn.Parameter(torch.zeros(dim)).to(device)\n    self.eps = eps\n\n  def forward(self, x):\n    mean = x.mean(-1,keepdim=True)\n    var = x.var(-1, unbiased=False, keepdim=True)\n    xhat = (x-mean)/torch.sqrt(var+self.eps)\n    return (self.gamma * xhat) +self.beta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.465372Z","iopub.execute_input":"2025-03-20T11:27:51.465679Z","iopub.status.idle":"2025-03-20T11:27:51.481871Z","shell.execute_reply.started":"2025-03-20T11:27:51.465652Z","shell.execute_reply":"2025-03-20T11:27:51.480895Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class FeedForward(nn.Module):\n  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n    super().__init__()\n    self.feed_forward_layer = nn.Sequential(\n      nn.Linear(input_dim, hidden_dim),\n      nn.GELU(),\n      nn.Linear(hidden_dim, output_dim),\n      nn.Dropout(dropout)\n    )\n\n  def forward(self, x):\n    return self.feed_forward_layer(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.483018Z","iopub.execute_input":"2025-03-20T11:27:51.483393Z","iopub.status.idle":"2025-03-20T11:27:51.498898Z","shell.execute_reply.started":"2025-03-20T11:27:51.483362Z","shell.execute_reply":"2025-03-20T11:27:51.497682Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class Block(nn.Module):\n  def __init__(self,embed_dim,heads=4):\n    super().__init__()\n    self.layer_norm1 = LayerNorm1D(embed_dim)\n    self.layer_norm2 = LayerNorm1D(embed_dim)\n    self.masked_multi_head_attn =  MaskedMultiHeadAttention(embed_dim, heads = 4)\n    self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n\n  def forward(self, x):\n    x = x + self.masked_multi_head_attn(self.layer_norm1(x))\n    x = x + self.feed_forward_layer(self.layer_norm2(x))\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.501849Z","iopub.execute_input":"2025-03-20T11:27:51.502158Z","iopub.status.idle":"2025-03-20T11:27:51.519091Z","shell.execute_reply.started":"2025-03-20T11:27:51.502131Z","shell.execute_reply":"2025-03-20T11:27:51.517934Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def apply_rope(x):\n    _, seq_len, dim = x.shape\n    pos = torch.arange(seq_len, device=device).float()\n    assert dim % 2 == 0, \"Embedding dimension must be even for RoPE\"\n    theta = 1.0 / (10000 ** (2 * (torch.arange(dim // 2, device=device).float() / dim)))\n    angles = torch.outer(pos, theta)\n    sin_angles = torch.sin(angles)\n    cos_angles = torch.cos(angles)\n    x_real, x_imag = torch.chunk(x, 2, dim=-1)\n    x_rotated = torch.cat([\n        x_real * cos_angles - x_imag * sin_angles,\n        x_real * sin_angles + x_imag * cos_angles\n    ], dim=-1)\n    return x_rotated","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.520764Z","iopub.execute_input":"2025-03-20T11:27:51.521108Z","iopub.status.idle":"2025-03-20T11:27:51.532671Z","shell.execute_reply.started":"2025-03-20T11:27:51.521079Z","shell.execute_reply":"2025-03-20T11:27:51.531549Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"class AutoRegressiveModel(nn.Module):\n  def __init__(self, embed_dim, vocab_size, block_size = BLOCK_SIZE, heads=4, num_layers=4):\n    super().__init__()\n    self.block = nn.Sequential(*[Block(embed_dim,heads) for _ in range(num_layers)])\n    self.embedding = nn.Embedding(vocab_size, embed_dim)\n    self.final_layer_norm = LayerNorm1D(embed_dim)\n    self.final_layer = nn.Linear(embed_dim, vocab_size)\n\n  def forward(self, x, targets = None):\n    _, T = x.shape\n    x_emb = self.embedding(x)\n    x_pos_emb = apply_rope(x_emb)\n    x = x_emb + x_pos_emb\n    block_output = self.block(x)\n    x_out = self.final_layer_norm(block_output)\n    return self.final_layer(x_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.533905Z","iopub.execute_input":"2025-03-20T11:27:51.534286Z","iopub.status.idle":"2025-03-20T11:27:51.551998Z","shell.execute_reply.started":"2025-03-20T11:27:51.534245Z","shell.execute_reply":"2025-03-20T11:27:51.550930Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def load_text_from_folder(folder_path):\n    all_text = []\n    for root, _, files in os.walk(folder_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                with open(file_path, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                    all_text.append(content)\n            except Exception as e:\n                print(f\"Skipping {file_path}: {e}\")\n    return \" \".join(all_text)\n\nfolder_path = r\"/kaggle/input/tamil-wikipedia-articles/train/train\"\nfull_corpus = load_text_from_folder(folder_path)\nprint(len(full_corpus))\n# output_file = \"tamil_corpus.txt\"\n# with open(output_file, \"w\", encoding=\"utf-8\") as f:\n#     f.write(full_corpus)\n\n#full_corpus = full_corpus[:10000]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:51.553278Z","iopub.execute_input":"2025-03-20T11:27:51.553581Z","iopub.status.idle":"2025-03-20T11:27:54.922464Z","shell.execute_reply.started":"2025-03-20T11:27:51.553546Z","shell.execute_reply":"2025-03-20T11:27:54.921296Z"}},"outputs":[{"name":"stdout","text":"143818754\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sp = spm.SentencePieceProcessor(model_file=\"/kaggle/input/sp/pytorch/default/1/tamil_spm.model\")\ndata = sp.encode(full_corpus, out_type=int)\nvocab_size = sp.get_piece_size()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T11:27:54.923804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AutoRegressiveDataset(Dataset):\n  def __init__(self,data, block_size):\n    self.data = data\n    self.block_size = block_size\n\n  def __len__(self):\n    return len(self.data)-self.block_size\n\n  def __getitem__(self,idx):\n    X= self.data[idx:idx+self.block_size]\n    y= self.data[idx+1:idx+self.block_size+1]\n    return torch.tensor(X).to(device),torch.tensor(y).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = AutoRegressiveDataset(data,BLOCK_SIZE)\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size= BLOCK_SIZE, heads = 4).to(device)\nif os.path.exists(\"/kaggle/input/tamilllm/pytorch/default/1/tamil_llm.pth\"):\n    model.load_state_dict(torch.load(\"/kaggle/input/tamilllm/pytorch/default/1/tamil_llm.pth\")) \noptimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(model: nn.Module, optimizer: torch.optim, criterion: nn.Module, dataloader: DataLoader, epochs: int):\n\n  for epoch in range(epochs):\n    model.train()\n    epoch_loss = 0.0\n    for X,y in dataloader:\n  \n      optimizer.zero_grad()\n\n      outputs = model(X)\n      B, T, _ = outputs.shape\n      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n      loss.backward()\n      optimizer.step()\n      epoch_loss += loss.item()\n    torch.save(model.state_dict(), \"tamil_llm.pth\")\n    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def val(model: nn.Module,dataloader: DataLoader):\n  model.eval()\n  val_loss = 0.0\n  with torch.no_grad():\n    for X,y in dataloader:\n      outputs = model(X)\n      B, T, _ = outputs.shape\n      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n      val_loss += loss.item()\n    print(f\"Loss: {val_loss / len(dataloader):.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train(model, optimizer, criterion, train_loader, 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#val(model,val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate(model: torch.nn.Module, start_seq: str = \"அவள் வீட்டுக்கு சென்றாள்\", epochs=100):\n    content_tokens = sp.encode(start_seq, out_type=int)  \n    for _ in range(epochs):\n        value = torch.tensor(content_tokens[-BLOCK_SIZE:]).unsqueeze(0).to(device)\n\n        outputs = model(value).squeeze(0)\n        probs = torch.softmax(outputs[-1], dim=-1)\n        next_token_id = torch.multinomial(probs, 1).item()\n        \n        content_tokens.append(next_token_id)  \n\n    return sp.decode(content_tokens) \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"content = generate(model, epochs = 100)\n''.join(content)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#torch.save(model.state_dict(), \"tamil_llm.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(''.join(content))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
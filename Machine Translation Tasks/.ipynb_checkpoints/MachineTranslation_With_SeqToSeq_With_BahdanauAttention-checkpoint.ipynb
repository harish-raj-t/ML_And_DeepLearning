{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9f56267-e047-408b-b700-bfd5656dd38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "path = r'C:\\\\Users\\\\harish-4072\\\\Downloads\\\\eng_french.csv'\n",
    "df = pd.read_csv(path, names=['English','French'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5cf0886-675a-40c5-9619-a987c0a4c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = text.split()  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1667dd2-4968-4760-8d63-c92c560667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = df['English'].dropna().apply(preprocess_text)\n",
    "english_vocab = Counter([token for sentence in english_sentences for token in sentence])\n",
    "\n",
    "french_sentences = df['French'].dropna().apply(preprocess_text)\n",
    "french_vocab = Counter([token for sentence in french_sentences for token in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc83af3-e8c3-4d21-bfa2-7fc287c5839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_to_id = {token: idx + 1 for idx, token in enumerate(english_vocab)}  # Start from 1 to reserve 0 for padding\n",
    "french_token_to_id = {token: idx + 3 for idx, token in enumerate(french_vocab)}\n",
    "\n",
    "english_token_to_id['<PAD>'] = 0\n",
    "\n",
    "french_token_to_id['<PAD>'] = 0\n",
    "french_token_to_id['<SOS>'] = 1\n",
    "french_token_to_id['<EOS>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6039b86c-6d89-4c69-99da-5ed3a7b7fa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_id_to_token= {value:key for key,value in french_token_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b24936-8c74-4a24-a580-1554e1417daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_vocab_size = len(english_token_to_id)\n",
    "french_vocab_size = len(french_token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388e6b58-a436-4f5b-a1a6-bcd61c2a405a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens,token_to_id):\n",
    "    return [token_to_id.get(token,0) for token in tokens]\n",
    "\n",
    "english_sequences = english_sentences.apply(lambda x: tokenize_text(x, english_token_to_id))\n",
    "french_sequences = french_sentences.apply(lambda x: tokenize_text(x, french_token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc48382b-5a52-4c91-9a7d-5132007a48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_sos_eos(tokens):\n",
    "    return [1]+tokens+[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aba5ed7-bf55-48d5-a86b-e4cc33d353ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sequences = french_sequences.apply(lambda x: add_sos_eos(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817ec228-6b92-49e7-9d32-b3b44ab5aa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self,english_sequences,french_sequences):\n",
    "        self.english_sequences = english_sequences\n",
    "        self.french_sequences = french_sequences\n",
    "        assert len(self.english_sequences) == len(self.french_sequences)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.english_sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        X= self.english_sequences[idx]\n",
    "        y= self.french_sequences[idx]\n",
    "        return torch.tensor(X,dtype=torch.long),torch.tensor(y,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "446f7892-79c8-44c8-8499-ca14d0ccfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    X,y = zip(*batch)\n",
    "    X_lengths = [len(item) for item in X]\n",
    "    y_lengths = [len(item) for item in y]\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    return X_padded, y_padded, X_lengths, y_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa88e90-72cd-4224-b414-7a9f43cb3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_temp, french_temp = english_sequences[100000:150000].reset_index(drop=True), french_sequences[100000:150000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413527a1-4b0f-47b8-a2f0-a50cfd5d4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentencesDataset(english_temp,french_temp)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False,collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f307a2b2-d3e4-464d-b975-f9b0282daa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.5\n",
    "SRC_VOCAB_SIZE = english_vocab_size  \n",
    "PAD_IDX = 0 \n",
    "TRG_VOCAB_SIZE = french_vocab_size  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3dc8b5d-ed00-4473-a5dc-37a8fa12172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout,padding_idx):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=padding_idx)\n",
    "#         self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         self.fc = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "\n",
    "#     def forward(self, src, lengths):\n",
    "#         #src - batch*l\n",
    "#         #embedded - batch*l*embedding\n",
    "#         #outputs - batch*l*hidden_dim*2\n",
    "#         #final_hidden - 1*batch*hidden_dim (same as decoder GRU hidden size)\n",
    "#         embedded = self.dropout(self.embedding(src)) \n",
    "#         packed_input = pack_padded_sequence(embedded, lengths, batch_first=True, enforce_sorted=False)\n",
    "#         packed_output, hidden = self.rnn(packed_input) \n",
    "#         outputs, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "#         final_hidden = torch.tanh(self.fc(outputs[:,-1,:]))  \n",
    "#         # final_hidden = torch.tanh(\n",
    "#         #     self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "#         # )\n",
    "#         return outputs, final_hidden.unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8b9722f-8c82-404f-be39-d4ab927146e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers, dropout,padding_idx):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim*2,hidden_dim)\n",
    "\n",
    "    def forward(self, src):\n",
    "        #src - batch*l\n",
    "        #embedded - batch*l*embedding\n",
    "        #outputs - batch*l*hidden_dim*2\n",
    "        #final_hidden - 1*batch*hidden_dim (same as decoder GRU hidden size)\n",
    "        embedded = self.dropout(self.embedding(src)) \n",
    "        outputs, hidden = self.rnn(embedded) \n",
    "        # final_hidden = torch.tanh(self.fc(outputs[:,-1,:]))  \n",
    "        final_hidden = torch.tanh(\n",
    "            self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        )\n",
    "        return outputs, final_hidden.unsqueeze(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bc94aa2-c9a7-4f0b-86b6-cfcabc51e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, encoder_hdim, decoder_hdim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear((encoder_hdim*2)+decoder_hdim, decoder_hdim)\n",
    "        self.fc = nn.Linear(decoder_hdim,1,bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        #encoder_outputs - batch*l*hidden_dim*2\n",
    "        #decoder_hidden - 1*batch*hidden_dim \n",
    "        #decoder_hidden_expanded - batch*l*hidden_dim\n",
    "        #a - batch*l* hidden_dim*2+hidden_dim\n",
    "        #energy - batch*l*hidden_dim\n",
    "        #output - batch*l*1\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        decoder_hidden = decoder_hidden.permute(1,0,2)\n",
    "        decoder_hidden_expanded = decoder_hidden.repeat(1, src_len, 1)  \n",
    "        a = torch.cat([decoder_hidden_expanded,encoder_outputs], dim=-1)\n",
    "        energy = torch.tanh(self.linear(a))\n",
    "        \n",
    "        return torch.softmax(self.fc(energy),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5225a46e-4d30-4d5b-82be-5743bde380dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention2(nn.Module):\n",
    "    def __init__(self, encoder_hdim, decoder_hdim, attention_weight):\n",
    "        super(Attention, self).__init__()\n",
    "        self.Wa = nn.Linear((encoder_hdim*2), attention_weight)\n",
    "        self.Ua = nn.Linear(decoder_hdim, attention_weight)\n",
    "        self.fc = nn.Linear(attention_weight,1,bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden):\n",
    "        #encoder_outputs - batch*l*hidden_dim*2\n",
    "        #decoder_hidden - 1*batch*hidden_dim \n",
    "        \n",
    "        #a - batch*l* hidden_dim*2+hidden_dim\n",
    "        #energy - batch*l*hidden_dim\n",
    "        #output - batch*l*1\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        decoder_hidden = decoder_hidden.permute(1,0,2)\n",
    "        weighted = torch.tanh(self.Wa(encoder_outputs) + self.Ua(decoder_hidden))\n",
    "        attention_weights = self.fc(weighted)\n",
    "        return torch.softmax(self.fc(attention_weights),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9461e531-f8f6-482d-8685-ce309375335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim, emb_dim, encoder_hdim, decoder_hdim,dropout = 0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.GRU(emb_dim+(encoder_hdim*2), decoder_hdim, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear((decoder_hdim+(encoder_hdim*2)+emb_dim),output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src, encoder_outputs, hidden):\n",
    "        #src = batch\n",
    "        #encoder_outputs - batch*l*hidden_dim*2\n",
    "        #hidden - 1*batch*hidden_dim \n",
    "        #embedded = batch*1*emd_dim\n",
    "        #attention_weights - batch*l*1\n",
    "        #weighted - batch*1*hidden_dim*2\n",
    "        #energy = batch*1*emd_dim.cat(batch*1*hidden_dim*2) = batch*1*(emd_dim+hidden_dim*2)\n",
    "        #output, hidden = batch * 1 * hidden_dim\n",
    "        #torch.cat((output,embedded,weighted.permute(0,2,1)) = batch*1*hidden_dim+(hidden_dim*2)+emb_dim\n",
    "        #predictions = batch*1*target_vocab_size\n",
    "        #hidden = 1*batch*hidden_dim\n",
    "        embedded = self.dropout(self.embedding(src.unsqueeze(1)))\n",
    "        attn_weights = self.attention(encoder_outputs, hidden)\n",
    "        weighted = torch.bmm(attn_weights.permute(0,2,1),encoder_outputs)\n",
    "        rnn_input  = torch.cat((embedded,weighted),dim=2)\n",
    "        output,hidden = self.rnn(rnn_input ,hidden)\n",
    "        predictions = self.fc(torch.cat((output,embedded,weighted),dim=2))\n",
    "        return predictions.squeeze(1), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97f5a9d2-2f4b-4d41-8770-a826391c916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqToSeq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(SeqToSeq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        #src = batch*l\n",
    "        #trg = batch*MAX_LENGTH\n",
    "        #src_len = batch*l\n",
    "        #encoder_outputs - batch*l*hidden_dim*2\n",
    "        #hidden - 1*batch*hidden_dim \n",
    "        #outputs - batch*MAX_LENGTH*TRG_VOCAB_SIZE\n",
    "        #input - batch\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        outputs = torch.zeros(batch_size, trg_len, TRG_VOCAB_SIZE)\n",
    "        input = trg[:, 0]\n",
    "        for t in range(1,trg_len):\n",
    "            #predictions - batch*1*target_vocab_size\n",
    "            #hidden - 1*batch*hidden_dim\n",
    "            #outputs - batch*MAX_LENGTH*TRG_VOCAB_SIZE\n",
    "            #top1, input - batch\n",
    "            predictions, hidden = self.decoder(input, encoder_outputs, hidden)\n",
    "            outputs[:, t, :] = predictions  \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = predictions.argmax(1) \n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a11fb04-46a9-4b40-b96f-f5e265b62598",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    input_dim=SRC_VOCAB_SIZE,\n",
    "    emb_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    padding_idx = 0\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a85fda2-65e6-4c93-8e65-33d693a40f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = BahdanauAttention(\n",
    "    encoder_hdim= HIDDEN_DIM, \n",
    "    decoder_hdim=HIDDEN_DIM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51becfb0-4eed-48a7-a3c8-8fc204ee61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(\n",
    "    output_dim=TRG_VOCAB_SIZE,\n",
    "    emb_dim=EMBEDDING_DIM,\n",
    "    encoder_hdim=HIDDEN_DIM,\n",
    "    decoder_hdim=HIDDEN_DIM,\n",
    "    dropout = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348ecee6-eb55-4a98-9b26-3dbed40755cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y,_,_ in train_loader:\n",
    "#     outputs, hidden = encoder(X)\n",
    "# print(outputs.shape, hidden.shape)\n",
    "# print(attention(outputs,hidden).shape)\n",
    "# input = torch.randint(0, 100, (1,32))\n",
    "# input.squeeze(0).shape\n",
    "# a,b = decoder(input.squeeze(0), outputs,hidden)\n",
    "# a.shape,b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bf548c5-d0b5-41a6-8ffd-f1d4eeb48bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harish-4072\\AppData\\Local\\Temp\\ipykernel_27980\\1084942259.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"seq2seq_model_weights_attention.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model = SeqToSeq(encoder, decoder)\n",
    "if os.path.exists(\"seq2seq_model_weights_attention.pth\"):\n",
    "    model.load_state_dict(torch.load(\"seq2seq_model_weights_attention.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4e0ae97-a1e6-41bb-a441-af8b51d81c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 50,638,676 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ca4e334-bf45-45bd-83c1-a1a46a88b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 7\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08db6167-65a0-4d2c-84f4-5444f67d964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18247d51-c344-445e-8933-c8697673411d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/7, Loss: 3.6430\n",
      "Epoch: 2/7, Loss: 2.9698\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg, src_lengths,_ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg,  0.5)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])  \n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        torch.save(model.state_dict(), \"seq2seq_model_weights_attention.pth\")\n",
    "    print(f\"Epoch: {epoch + 1}/{EPOCHS}, Loss: {epoch_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b2e975-12e5-4990-9bd5-c1237b137355",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "epoch_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, trg, src_lengths, _ in val_loader:\n",
    "            \n",
    "        output = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])  # Ignore <sos> token\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(epoch_loss / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca2ed4-e7ea-4f83-b883-7683db980901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, src, trg_vocab, max_len=50):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encoder(src)\n",
    "\n",
    "        # Start with <sos> token\n",
    "        trg_vocab_size = TRG_VOCAB_SIZE\n",
    "        input = torch.tensor([1])\n",
    "        print(input.shape)\n",
    "        predictions = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden = model.decoder(input, encoder_outputs, hidden)\n",
    "            print(output.shape)\n",
    "            top1 = output.argmax(1)  # Get the token with highest probability\n",
    "            predictions.append(top1.item())\n",
    "            print(top1)\n",
    "            if top1.item() == trg_vocab['<EOS>']:\n",
    "                break\n",
    "\n",
    "            input = top1  # Use the predicted token as input for the next step\n",
    "    return [french_id_to_token[idx] for idx in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53415baf-4aef-40d7-82c3-5e7bd997b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I like you\"\n",
    "sentence2 = \"I like you well good\"\n",
    "sentence = preprocess_text(sentence)\n",
    "sentence = tokenize_text(sentence, english_token_to_id)\n",
    "sentence2 = preprocess_text(sentence2)\n",
    "sentence2 = tokenize_text(sentence2, english_token_to_id)\n",
    "sentence = sentence\n",
    "sentence2 = [1] + sentence2 + [2]\n",
    "input = torch.tensor([sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c296d0f-fb15-4502-94c7-2bd1914644c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer(model,input,french_token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4579f-a60b-4307-87d3-2d0e36b6eca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

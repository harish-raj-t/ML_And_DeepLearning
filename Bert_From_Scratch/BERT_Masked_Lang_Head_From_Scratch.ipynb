{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a779d41e-6c21-4393-8c07-43ec2494082a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x299b3730>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence,pack_padded_sequence, pad_packed_sequence\n",
    "import os\n",
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "719e84a3-73f0-45ba-89f0-77488e432358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emd_dim, heads=4, dropout = 0.2):\n",
    "        super().__init__()\n",
    "        assert emd_dim % heads == 0\n",
    "        self.heads = heads\n",
    "        self.head_dim = emd_dim//heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n",
    "        self.output = nn.Linear(emd_dim,emd_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_masking(attn_scores, padding_mask):\n",
    "        col_mask = padding_mask[:, None, None, :]\n",
    "        attn_scores.masked_fill_((col_mask == 0), float('-inf'))\n",
    "        return attn_scores\n",
    "\n",
    "    def forward(self, x, padding_mask=None, attn_mask=False, kv_cache = None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.multiHead(x)\n",
    "        q, k, v = torch.chunk(qkv,3,dim=-1)\n",
    "        q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        if attn_mask:\n",
    "            tril = torch.tril(torch.ones(T,T))\n",
    "            attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n",
    "        if padding_mask is not None:\n",
    "            attn_scores = self.add_masking(attn_scores, padding_mask)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        attn_probs_drop = self.dropout(attn_probs)\n",
    "        attn_output = torch.matmul(attn_probs_drop,v)\n",
    "        fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n",
    "        return self.output(fn_attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e8af5e-7064-44d6-b756-f683303dda15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1D(nn.Module):\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    super(LayerNorm1D, self).__init__()\n",
    "    self.gamma = nn.Parameter(torch.ones(dim))\n",
    "    self.beta = nn.Parameter(torch.zeros(dim))\n",
    "    self.eps = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(-1,keepdim=True)\n",
    "    var = x.var(-1, unbiased=False, keepdim=True)\n",
    "    xhat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "    return (self.gamma * xhat) +self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dca0f7b-9443-492a-a665-11176459212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n",
    "    super().__init__()\n",
    "    self.feed_forward_layer = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(hidden_dim, output_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.feed_forward_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9b827dd-09a5-4c32-9728-ed09e92bed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,embed_dim, heads=4):\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = LayerNorm1D(embed_dim)\n",
    "        self.layer_norm2 = LayerNorm1D(embed_dim)\n",
    "        self.multi_head_attn =  MultiHeadAttention(embed_dim, heads)\n",
    "        self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n",
    "    \n",
    "    def forward(self, x, padding_mask):\n",
    "        x = x + self.multi_head_attn(self.layer_norm1(x), padding_mask)\n",
    "        x = x + self.feed_forward_layer(self.layer_norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d36309-54f3-44a9-a2e3-76102766d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, src_max_length, heads = 4, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(embed_dim,heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, padding_mask = None):\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x, padding_mask = padding_mask) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b257f5d9-1b81-41e7-8ec9-1a1203832ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, max_length, segment_needed=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_embedding = nn.Embedding(max_length, embed_dim)\n",
    "        self.segment_needed = segment_needed\n",
    "        if self.segment_needed:\n",
    "            \n",
    "            #self.segmentation_embedding\n",
    "            pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emd = self.embedding(x)\n",
    "        x_pos_emd = self.positional_embedding(torch.arange(x.shape[1]))\n",
    "        x = x_emd + x_pos_emd\n",
    "        if self.segment_needed:\n",
    "            #self.segmentation_embedding\n",
    "            pass\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc46f9af-0a7f-4db7-ab89-235c4a54b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, max_length, heads = 4, num_layers = 4):\n",
    "        super().__init__()\n",
    "        self.embedding = EmbeddingBlock(embed_dim, vocab_size, max_length)\n",
    "        self.encoder = Encoder(embed_dim, max_length, heads = 4, num_layers = 4)\n",
    "        self.linear = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, mask):\n",
    "        src = self.embedding(src)\n",
    "        encoder_outputs = self.encoder(src, mask)\n",
    "        output = self.linear(encoder_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fa42ed7-cd1c-405f-9299-11a64cee91b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\\\Users\\\\harish-4072\\\\Downloads\\\\eng_french.csv'\n",
    "df = pd.read_csv(path, names=['English','French'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cbe2909-e67b-497b-be97-5d706367af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  \n",
    "    tokens = text.split()  \n",
    "    return tokens if len(tokens) >= 5 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d10f668-1e89-4c76-a54a-a6b18048996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df['English'].dropna().apply(preprocess_text)\n",
    "df = df.dropna()\n",
    "vocab = Counter([token for sentence in df for token in sentence])\n",
    "token_to_id = {token: idx + 2 for idx, token in enumerate(vocab)} \n",
    "token_to_id['<PAD>'] = 0\n",
    "token_to_id['<MASK>'] = 1\n",
    "id_to_token= {value:key for key,value in token_to_id.items()}\n",
    "vocab_size = len(id_to_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69457503-f2db-43e1-b4cb-738cf75ec586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokens,token_to_id):\n",
    "    return [token_to_id.get(token,0) for token in tokens]\n",
    "\n",
    "df_sentences = df.apply(lambda x: tokenize_text(x, token_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3494582c-bd1b-4aab-997a-2aa1617c498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = df_sentences.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "116029f9-82a1-42d6-b0b3-b9b6819c08d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = tokenize_text(preprocess_text(\"Hello, this is a good thing in life.\"), token_to_id)\n",
    "sentence2 = tokenize_text(preprocess_text(\"I went to play cricket in rainy weather\"), token_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9107438b-6b79-4891-9a2e-20e508717462",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1_masked = sentence1\n",
    "sentence2_masked = sentence2\n",
    "sentence1_masked[2]= 1\n",
    "sentence2_masked[4] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b62dbd41-9871-4fd4-a208-6744c0165904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentencesDataset(Dataset):\n",
    "    def __init__(self, sequences, mask_prob = 0.20):\n",
    "        self.sequences = sequences\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        masked = torch.full((len(self.sequences[idx]),), self.mask_prob)\n",
    "        masked_idx = torch.bernoulli(masked)\n",
    "        masked_idx_bool = masked_idx.bool()\n",
    "        X= torch.tensor(self.sequences[idx],dtype=torch.long)\n",
    "        X_cloned = X.clone()\n",
    "        X_cloned[masked_idx_bool] = 1\n",
    "        return X_cloned, X, masked_idx_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d44205-cfe7-4eb5-8594-019c7073c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j,k in SentencesDataset(df_sentences):\n",
    "#     print(i, j, type(k))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fba385f-839d-439d-b7f0-45c9aa3b01d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    X, y, mask = zip(*batch)\n",
    "    X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
    "    y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    mask = pad_sequence(mask, batch_first=True, padding_value=False)\n",
    "    padding_mask = (X_padded != 0) \n",
    "    return X_padded, y_padded, padding_mask, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29f754c0-51de-4a6d-8d9a-3c5561b5ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sentences = df_sentences[:50000].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37ce8f16-e08f-4f6f-bd85-36d2a75002b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentencesDataset(df_sentences)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True,collate_fn = collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False,collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac8a73bd-f1f7-41c7-8d39-f652b0efe50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X_padded, y_padded, padding_mask, masked in train_loader:\n",
    "#     print(X_padded, y_padded, padding_mask, type(masked))\n",
    "#     logits = torch.randn(2,X_padded.shape[1],10)\n",
    "#     labels = torch.randint(1,2,(2, X_padded.shape[1]))\n",
    "#     # print(logits.shape, labels.shape, type(masked))\n",
    "#     # masked_logits = logits[masked]  \n",
    "#     # masked_labels = labels[masked]\n",
    "#     # print(masked_logits, masked_labels)\n",
    "#     mask_tensor = torch.stack(masked)  \n",
    "\n",
    "#     indices = torch.nonzero(mask_tensor, as_tuple=True)\n",
    "#     print(mask_tensor, indices)\n",
    "    \n",
    "#     logits = logits[indices[0], indices[1]]  # Shape [num_selected, 10]\n",
    "#     labels = y_padded[indices[0], indices[1]] \n",
    "#     print(logits, labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f426291-dc48-462f-adbc-16302111c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 128\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.5\n",
    "VOCAB_SIZE = vocab_size  \n",
    "PAD_IDX = 0 \n",
    "MAX_LEN = max(df_sentences.apply(len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05fade9f-4ce1-41e9-b2a2-f8925022a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harish-4072\\AppData\\Local\\Temp\\ipykernel_34588\\2761104732.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"Bert_mask_pred_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model = BERT( embed_dim = EMBEDDING_DIM,  vocab_size = VOCAB_SIZE, max_length = MAX_LEN, heads = 4, num_layers = 4)\n",
    "if os.path.exists(\"Bert_mask_pred_model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"Bert_mask_pred_model.pth\")) \n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff65a900-53dd-47b9-974a-3773e8fab3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model :nn.Module, criterion: nn.Module, optimizer: torch.optim, train_data: DataLoader, epochs: int = 4):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for X, y, padding_mask, masked in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X, padding_mask)\n",
    "            indices = torch.nonzero(masked, as_tuple=True)\n",
    "            logits = outputs[indices[0], indices[1]] \n",
    "            labels = y[indices[0], indices[1]] \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        torch.save(model.state_dict(), \"Bert_mask_pred_model.pth\")\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(train_data):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79507f88-05d9-417d-84f2-b9ef4965f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Loss: 3.6335\n",
      "Epoch: 2/3, Loss: 3.5493\n",
      "Epoch: 3/3, Loss: 3.4445\n"
     ]
    }
   ],
   "source": [
    "train(model, criterion, optimizer, train_loader, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b3511f5-8f7c-4c40-9b56-5f999fbd2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model :nn.Module, criterion: nn.Module, optimizer: torch.optim, val_data: DataLoader):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y, padding_mask, masked in val_data:\n",
    "            outputs = model(X, padding_mask)\n",
    "            indices = torch.nonzero(masked, as_tuple=True)\n",
    "            logits = outputs[indices[0], indices[1]] \n",
    "            labels = y[indices[0], indices[1]] \n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "        print(f\"Loss: {val_loss / len(val_data):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5010f400-0a74-4051-a887-20a10657e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.5335\n"
     ]
    }
   ],
   "source": [
    "val(model, criterion, optimizer, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b85923c-a12d-4161-b211-4146dbbe8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def generate(model: nn.Module, input_ids: torch.Tensor, padding_mask: torch.Tensor = None, top_k: int = 5):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        outputs = model(input_ids, padding_mask) \n",
    "        masked_positions = (input_ids == 1)  \n",
    "        indices = torch.nonzero(masked_positions, as_tuple=True)  \n",
    "        masked_logits = outputs[indices[0], indices[1], :]  \n",
    "        top_k_preds = torch.topk(masked_logits, top_k, dim=-1) \n",
    "        return top_k_preds.indices  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "335c2dee-ffa9-47a7-a89d-e194073e0064",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([sentence1_masked, sentence2_masked])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd93d63e-6980-4582-89d3-43212827f673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  21,  920,   48,   19, 1267]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(model, torch.tensor([sentence1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08d8b4b5-5f32-4ceb-aa11-2dab260ace03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'these'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token[1316]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40b44ebe-f584-4b32-92da-e34fd243ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = tokenize_text(preprocess_text(\"Are you an idiot in life\"), token_to_id)\n",
    "# sentence2 = tokenize_text(preprocess_text(\"I went to play cricket in rainy weather\"), token_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2eb4f2e9-13c4-4be8-bcf6-987cbb47a5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2295, 81, 1, 4, 23, 220, 7, 267], [2, 290, 10, 128, 1, 7, 4525, 2495])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1_masked, sentence2_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcd11cb5-d289-46b6-8840-26514dc4bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1[1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d04a4-8851-480f-8ceb-2fbc379341d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa60c86-5e66-488a-b8ee-8038eaac499b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

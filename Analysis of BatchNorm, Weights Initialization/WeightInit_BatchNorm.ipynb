{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17817d1b-257e-4dc1-8ae8-8e2a91c77f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f42642dd-d198-48a8-b94b-9cbc3d7c036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the training and test sets\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoader for batching the data\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55e4c313-e689-4ad8-a912-ab6baa4f824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6795],\n",
      "        [-0.7274],\n",
      "        [-0.7503],\n",
      "        [-0.5828],\n",
      "        [-0.9010],\n",
      "        [-0.5957],\n",
      "        [-0.8206],\n",
      "        [-0.7345],\n",
      "        [-0.7764],\n",
      "        [-0.6936],\n",
      "        [-0.7567],\n",
      "        [-0.6417],\n",
      "        [-0.7193],\n",
      "        [-0.8365],\n",
      "        [-0.6733],\n",
      "        [-0.8073],\n",
      "        [-0.6327],\n",
      "        [-0.8062],\n",
      "        [-0.7812],\n",
      "        [-0.5953],\n",
      "        [-0.6956],\n",
      "        [-0.8675],\n",
      "        [-0.7459],\n",
      "        [-0.7307],\n",
      "        [-0.6882],\n",
      "        [-0.7270],\n",
      "        [-0.7489],\n",
      "        [-0.8384],\n",
      "        [-0.8071],\n",
      "        [-0.8147],\n",
      "        [-0.8496],\n",
      "        [-0.6396],\n",
      "        [-0.5727],\n",
      "        [-0.6368],\n",
      "        [-0.8447],\n",
      "        [-0.6879],\n",
      "        [-0.8294],\n",
      "        [-0.7231],\n",
      "        [-0.6169],\n",
      "        [-0.7107],\n",
      "        [-0.7561],\n",
      "        [-0.6173],\n",
      "        [-0.5702],\n",
      "        [-0.7672],\n",
      "        [-0.8443],\n",
      "        [-0.6938],\n",
      "        [-0.8402],\n",
      "        [-0.6283],\n",
      "        [-0.7155],\n",
      "        [-0.7731],\n",
      "        [-0.7253],\n",
      "        [-0.8855],\n",
      "        [-0.7604],\n",
      "        [-0.8429],\n",
      "        [-0.8790],\n",
      "        [-0.8749],\n",
      "        [-0.8176],\n",
      "        [-0.8870],\n",
      "        [-0.7028],\n",
      "        [-0.7911],\n",
      "        [-0.6071],\n",
      "        [-0.7767],\n",
      "        [-0.6266],\n",
      "        [-0.7664],\n",
      "        [-0.5846],\n",
      "        [-0.8045],\n",
      "        [-0.6632],\n",
      "        [-0.7063],\n",
      "        [-0.6502],\n",
      "        [-0.7769],\n",
      "        [-0.6448],\n",
      "        [-0.8098],\n",
      "        [-0.7486],\n",
      "        [-0.6643],\n",
      "        [-0.7854],\n",
      "        [-0.6255],\n",
      "        [-0.8496],\n",
      "        [-0.6474],\n",
      "        [-0.8672],\n",
      "        [-0.7001],\n",
      "        [-0.5518],\n",
      "        [-0.7740],\n",
      "        [-0.6765],\n",
      "        [-0.5791],\n",
      "        [-0.5731],\n",
      "        [-0.8507],\n",
      "        [-0.7427],\n",
      "        [-0.7822],\n",
      "        [-0.8669],\n",
      "        [-0.5711],\n",
      "        [-0.8401],\n",
      "        [-0.6071],\n",
      "        [-0.7394],\n",
      "        [-0.7541],\n",
      "        [-0.8051],\n",
      "        [-0.7252],\n",
      "        [-0.8957],\n",
      "        [-0.8448],\n",
      "        [-0.7185],\n",
      "        [-0.6321],\n",
      "        [-0.8291],\n",
      "        [-0.8661],\n",
      "        [-0.7211],\n",
      "        [-0.7015],\n",
      "        [-0.8410],\n",
      "        [-0.5938],\n",
      "        [-0.7096],\n",
      "        [-0.6055],\n",
      "        [-0.8523],\n",
      "        [-0.7722],\n",
      "        [-0.7181],\n",
      "        [-0.6301],\n",
      "        [-0.6556],\n",
      "        [-0.8736],\n",
      "        [-0.7701],\n",
      "        [-0.7418],\n",
      "        [-0.7326],\n",
      "        [-0.7469],\n",
      "        [-0.7430],\n",
      "        [-0.8324],\n",
      "        [-0.8390],\n",
      "        [-0.8193],\n",
      "        [-0.8009],\n",
      "        [-0.4745],\n",
      "        [-0.8453],\n",
      "        [-0.8396],\n",
      "        [-0.7822],\n",
      "        [-0.8867]])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for batches, images in trainloader:\n",
    "    print(batches.view(128, -1).mean(1,keepdim=True))\n",
    "    print(images.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9eb254-45af-40c4-aa28-885b8b07593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1=0\n",
    "x_2 =0\n",
    "x_3=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2818da1-e045-4fba-bc6e-5a2d4d0f29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Tanh()  \n",
    "\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x_1 = x\n",
    "        x = self.fc2(x)\n",
    "        x_2 = x\n",
    "        x= self.fc3(x)\n",
    "        x_3 = x\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffda497-e03f-42ce-89bf-dc9a0c492afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011675c-8317-4cce-a3b1-122e3d9461d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    running_loss = 0.0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad() \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "        total += images.shape[0]\n",
    "        correct += (torch.argmax(output,dim=1)==labels).sum()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}')\n",
    "    print(f'Accuracy = {(correct/total)*100}')\n",
    "    losses.append(running_loss/len(trainloader)) \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "69200215-cab8-44d3-8a6d-d50fb013a6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer Statistics Analysis:\n",
      "--------------------------------------------------\n",
      "\n",
      "input layer:\n",
      "mean: -0.012586\n",
      "var: 0.969288\n",
      "min: -0.424213\n",
      "max: 2.821487\n",
      "\n",
      "fc1 layer:\n",
      "mean: -0.018136\n",
      "var: 1.423586\n",
      "min: -6.484151\n",
      "max: 5.996514\n",
      "\n",
      "fc2 layer:\n",
      "mean: -0.007757\n",
      "var: 0.450601\n",
      "min: -0.999995\n",
      "max: 0.999988\n",
      "\n",
      "fc3 layer:\n",
      "mean: -0.330184\n",
      "var: 0.817783\n",
      "min: -3.310901\n",
      "max: 3.213920\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(256, 10)  # Fixed the dimension mismatch\n",
    "        \n",
    "        # Initialize weights using Xavier/Glorot initialization\n",
    "        init.xavier_uniform_(self.fc1.weight,gain=1)\n",
    "        init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.activations = {}  # Store activations at each layer\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        self.activations['input'] = x\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        self.activations['fc1'] = x\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        self.activations['fc2'] = x\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        self.activations['fc3'] = x\n",
    "        \n",
    "        return x\n",
    "\n",
    "def analyze_layer_statistics(model, images):\n",
    "    \"\"\"Analyze mean and variance at each layer\"\"\"\n",
    "    output = model(images)\n",
    "    \n",
    "    stats = {}\n",
    "    for layer_name, activations in model.activations.items():\n",
    "        stats[layer_name] = {\n",
    "            'mean': torch.mean(activations).item(),\n",
    "            'var': torch.var(activations).item(),\n",
    "            'min': torch.min(activations).item(),\n",
    "            'max': torch.max(activations).item()\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Create model and get a batch of data\n",
    "model = SimpleNN()\n",
    "images, _ = next(iter(trainloader))\n",
    "\n",
    "# Analyze statistics before training\n",
    "stats = analyze_layer_statistics(model, images)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nLayer Statistics Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for layer_name, layer_stats in stats.items():\n",
    "    print(f\"\\n{layer_name} layer:\")\n",
    "    for stat_name, value in layer_stats.items():\n",
    "        print(f\"{stat_name}: {value:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4296229-65c9-4d71-bc04-e31ce54bf379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cb32b68-4adc-4131-8bc4-4630043a1d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Mean: 0.130660\n",
      "Calculated Variance: 0.094930\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Load the dataset with only ToTensor() (no normalization yet)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=False)\n",
    "\n",
    "# Accumulate sum and squared sum for mean and variance calculation\n",
    "mean = 0.0\n",
    "squared_sum = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "for images, _ in trainloader:\n",
    "    batch_samples = images.size(0)  # Number of images in the batch\n",
    "    num_samples += batch_samples\n",
    "    images = images.view(batch_samples, -1)  # Flatten the images to compute per-pixel stats\n",
    "    \n",
    "    mean += images.mean(dim=1).sum()  # Sum of means for this batch\n",
    "    squared_sum += (images ** 2).mean(dim=1).sum()  # Sum of squared values for this batch\n",
    "\n",
    "# Compute the final mean and variance\n",
    "mean /= num_samples\n",
    "var = (squared_sum / num_samples) - (mean ** 2)\n",
    "\n",
    "print(f\"Calculated Mean: {mean:.6f}\")\n",
    "print(f\"Calculated Variance: {var:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d887068b-883a-4c1e-91c8-8b23778c5ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics:\n",
      "Mean: 0.1307\n",
      "Std:  0.3081\n",
      "\n",
      "After normalization:\n",
      "Mean: -0.0079\n",
      "Std:  0.9904\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def calculate_normalization_stats(dataset_class, root='./data', **dataset_kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the mean and std of a dataset for normalization.\n",
    "    \n",
    "    Args:\n",
    "        dataset_class: The dataset class (e.g., datasets.MNIST)\n",
    "        root: Root directory for dataset\n",
    "        **dataset_kwargs: Additional arguments for dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean, std) of the dataset\n",
    "    \"\"\"\n",
    "    # Load dataset with only ToTensor transform initially\n",
    "    initial_transform = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = dataset_class(root=root, train=True, download=True, \n",
    "                          transform=initial_transform, **dataset_kwargs)\n",
    "    loader = DataLoader(dataset, batch_size=1000, num_workers=2)\n",
    "    \n",
    "    nimages = 0\n",
    "    mean = 0.\n",
    "    var = 0.\n",
    "    \n",
    "    # Calculate mean\n",
    "    for batch, _ in loader:\n",
    "        # Rearrange batch to be the shape of [B, C, W * H]\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        nimages += batch.size(0)\n",
    "        \n",
    "        # Update mean\n",
    "        mean += batch.mean(2).sum(0) \n",
    "    \n",
    "    mean /= nimages\n",
    "    \n",
    "    # Calculate variance\n",
    "    for batch, _ in loader:\n",
    "        batch = batch.view(batch.size(0), batch.size(1), -1)\n",
    "        var += ((batch - mean.unsqueeze(1).unsqueeze(2))**2).sum([0,2])\n",
    "    \n",
    "    std = torch.sqrt(var / (nimages * batch.size(2)))\n",
    "    \n",
    "    return mean.numpy(), std.numpy()\n",
    "\n",
    "# Calculate stats for MNIST\n",
    "mean, std = calculate_normalization_stats(datasets.MNIST)\n",
    "\n",
    "print(f\"Dataset statistics:\")\n",
    "print(f\"Mean: {mean[0]:.4f}\")\n",
    "print(f\"Std:  {std[0]:.4f}\")\n",
    "\n",
    "# Now you can create a transform with these values\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mean[0],), (std[0],))\n",
    "])\n",
    "\n",
    "# Verify the normalization\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size=1000)\n",
    "\n",
    "# Check statistics after normalization\n",
    "batch, _ = next(iter(loader))\n",
    "print(\"\\nAfter normalization:\")\n",
    "print(f\"Mean: {batch.mean():.4f}\")\n",
    "print(f\"Std:  {batch.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c41f1d48-6ab9-4752-8942-4baabecef4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a small dataset (2 features, 4 samples)\n",
    "num_points = 100  # Number of data points\n",
    "\n",
    "num_features = 28 * 28     # Number of features\n",
    "\n",
    "\n",
    "\n",
    "# Randomly generate data points\n",
    "\n",
    "# For generality, use values sampled from a uniform distribution between 0 and 10\n",
    "\n",
    "X = np.random.uniform(0, 10, size=(num_points, num_features))\n",
    "\n",
    "\n",
    "  \n",
    "scalar = StandardScaler()\n",
    "X = scalar.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b34e273c-5a73-41bc-ae07-2337ecd0fab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2688263138573217e-17,\n",
       " 1.0,\n",
       " array([[ 0.51459868, -0.05369564,  1.55959884, ...,  0.56548788,\n",
       "          1.10956482,  0.64754112],\n",
       "        [-0.99962407,  1.46620393, -1.10932716, ...,  0.3163979 ,\n",
       "          1.05531368,  0.78429994],\n",
       "        [ 0.18258542,  0.51780768, -0.24688498, ..., -1.01904053,\n",
       "         -0.06018188, -1.05716552],\n",
       "        ...,\n",
       "        [-0.29534408,  1.39244791, -0.93568659, ..., -0.26154742,\n",
       "         -0.60317134,  0.17969745],\n",
       "        [ 0.39737484, -0.12905242, -0.15637631, ...,  1.18319085,\n",
       "         -1.27346938,  1.54408932],\n",
       "        [-0.6447867 , -0.10099625, -0.58289847, ...,  0.47114734,\n",
       "         -1.82503895,  1.25656632]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(),X.std(),X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "142ba6cb-117d-4dde-809c-49eaf89d78b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (X):\n",
      "[[ 0.51459868 -0.05369564  1.55959884 ...  0.56548788  1.10956482\n",
      "   0.64754112]\n",
      " [-0.99962407  1.46620393 -1.10932716 ...  0.3163979   1.05531368\n",
      "   0.78429994]\n",
      " [ 0.18258542  0.51780768 -0.24688498 ... -1.01904053 -0.06018188\n",
      "  -1.05716552]\n",
      " ...\n",
      " [-0.29534408  1.39244791 -0.93568659 ... -0.26154742 -0.60317134\n",
      "   0.17969745]\n",
      " [ 0.39737484 -0.12905242 -0.15637631 ...  1.18319085 -1.27346938\n",
      "   1.54408932]\n",
      " [-0.6447867  -0.10099625 -0.58289847 ...  0.47114734 -1.82503895\n",
      "   1.25656632]]\n",
      "\n",
      "Weights (W):\n",
      "[[ 0.0703285  -0.09207616  0.00046712 ...  0.02878498  0.0114036\n",
      "   0.02639193]\n",
      " [ 0.05616378 -0.00774921 -0.01831405 ... -0.01155112  0.04080291\n",
      "  -0.06469014]\n",
      " [-0.11917771 -0.03050455 -0.12201427 ... -0.00902066 -0.0402458\n",
      "  -0.04474773]]\n",
      "\n",
      "Result (Y):\n",
      "[[-1.53217963 -1.18965792  0.41765409  1.67432202  0.99370468 -1.55805065\n",
      "   0.37785387  1.77745934  0.699951    0.24318296  3.93546067  1.2611915\n",
      "   1.62419509  0.7833101   0.06650761 -0.28537647  1.51216743 -0.17199809\n",
      "  -1.51771526 -1.40637181  1.04940595  0.49643467 -1.60202901 -1.57564246\n",
      "  -0.60150085 -1.01181009 -0.38433123 -0.1639282  -0.74936741  0.85492869\n",
      "   0.84833615 -0.73276578  0.67579529 -1.12297641  0.06987453 -0.1796488\n",
      "   1.51935651  0.91172295  1.58712129  1.11594894  0.33808039 -0.36588512\n",
      "   1.74632563 -0.97418987  0.65259753 -1.81592786  1.19433215 -1.09079028\n",
      "   0.26510383 -3.80888744  1.85389635 -2.91671781  0.08520961  0.56088095\n",
      "   0.34952434  0.25025731 -1.42476749 -0.17818528  0.94621965  1.25898594\n",
      "   0.50279848 -0.95515077 -1.89193334  0.86131512 -0.94189984 -1.9401279\n",
      "   0.274173    0.55882853 -1.72750699 -1.65184265  2.11499173 -0.08569106\n",
      "   0.47673531  1.69062318  0.74900397  1.21493833  0.11237482 -0.38334176\n",
      "   2.47386582  1.00282453 -0.48385527 -1.7268841   1.88302118  0.67829765\n",
      "  -0.4793621   1.06638587 -0.5004822  -0.5757562   0.74914995 -1.03474803\n",
      "  -0.82223572 -0.20942388  0.6885039  -2.97090694  0.8561102  -0.31357472\n",
      "  -2.25004503 -1.16450224 -1.19314011 -0.28812852]\n",
      " [-1.87290026 -1.06495506  1.06905054  2.75572189 -0.43069009 -2.25873061\n",
      "   2.41598078 -1.38145925  0.44187906 -0.51330323  2.23403489  3.48702562\n",
      "  -0.66709059 -1.45982245  1.43107721  1.47389582  2.9709205   0.06710945\n",
      "   0.8591759  -0.0405341   1.86554677 -1.52131082 -0.40226549  1.30411905\n",
      "  -1.89287091 -0.6819488   0.20981782 -0.65433884 -0.01101794 -0.32422893\n",
      "   0.52790661 -0.89316203 -0.00770503  2.76470463 -0.16813345  1.01177191\n",
      "  -2.530452    0.51587126 -2.26811681 -0.27103381  1.75104771  0.4385114\n",
      "   1.86913657  0.16554506  0.60325525  1.1625705  -0.50540087 -0.69990046\n",
      "  -1.650994   -0.44137754  1.32292853 -0.98289591 -0.74741545  2.12531167\n",
      "  -0.06812463  0.35743501  1.24502287 -1.64192925 -2.21192407  0.83367122\n",
      "  -0.61976565 -0.77659861  0.77639998 -0.40890831  0.30719558 -0.42168832\n",
      "  -1.82903537  2.1140534  -1.92511052  0.55917583 -2.05912489 -0.00697462\n",
      "  -0.38200071 -0.17692557 -3.56426057 -0.00573618  1.75946927 -0.62112477\n",
      "   0.63280303  1.09494644 -0.70216612 -0.27306812  2.53754951  1.25266662\n",
      "  -1.11735941  0.94284096 -1.56552426 -0.69818514  0.0522426  -0.2540641\n",
      "   0.85041787 -0.49411796 -0.11334892  0.01312373 -0.40031146  0.23415893\n",
      "  -0.48030471 -1.04016047 -0.46718912 -1.73800268]\n",
      " [-0.32962095 -2.1936237   2.4116311   2.51331044 -3.13420919 -0.21618403\n",
      "   0.47036541 -0.06753464  0.53608692 -1.29157783 -1.76927074 -2.13719429\n",
      "  -2.00103972 -1.97041746  1.18148132  2.07535956  0.37334483 -2.08840852\n",
      "  -0.70413823  0.30276898  2.89395811  1.75134773 -1.83659265  0.49468198\n",
      "   0.38509091  0.97015688  1.29345617 -0.60599785 -1.04500799  1.3111264\n",
      "  -0.05888338 -0.97653158 -0.6829147   2.07303641  2.59783834 -2.23456533\n",
      "  -1.63261467 -0.54205526 -3.05104549  0.81178271  1.19071165 -0.74052677\n",
      "   1.70143542  0.60961695 -0.52083861 -0.4577773   0.486928    0.95449094\n",
      "   1.33161722  0.29607905  2.3998178   0.25691938  1.51824067 -0.56055919\n",
      "  -1.17544627 -0.93405271  1.82726355  0.01487917  0.01849504 -1.46905242\n",
      "  -0.99599823 -0.36695062  1.08155533 -1.78255133 -0.77230992 -1.47248031\n",
      "  -0.9138817   1.16159948 -0.28749851 -2.52279176  0.15823923 -0.44533675\n",
      "  -1.17121154 -0.72545133  0.97003917  1.85183447 -0.76689835 -1.67616481\n",
      "   1.4610394   0.87938126  0.16923752  0.20093212  0.04023442  0.36234249\n",
      "   2.85413642  2.58793691 -2.12039149  1.19596681  0.27945505 -0.63578993\n",
      "   1.97729898  0.85252107 -2.25670699  0.40311864  0.22422635  2.94041719\n",
      "  -1.10183957 -0.60702348 -1.78003676  0.12416346]]\n",
      "\n",
      "Result after applying tanh (Y_tanh):\n",
      "[[-0.91079685 -0.83047275  0.39495229  0.93212081  0.75893758 -0.91510409\n",
      "   0.36084223  0.94442125  0.60433667  0.23849992  0.99923693  0.85139226\n",
      "   0.92523045  0.65460251  0.06640972 -0.27787379  0.90732293 -0.17032183\n",
      "  -0.90829867 -0.88672119  0.7815753   0.4593086  -0.92197341 -0.91791864\n",
      "  -0.53811668 -0.76650965 -0.36646297 -0.16247542 -0.63477141  0.69363557\n",
      "   0.69019923 -0.62475451  0.58877909 -0.80860172  0.06976104 -0.17774078\n",
      "   0.90858545  0.72195819  0.91970678  0.80615521  0.32576257 -0.35038724\n",
      "   0.9409559  -0.75053972  0.57341601 -0.94843084  0.83191761 -0.7971664\n",
      "   0.25906309 -0.99901721  0.95211156 -0.9941611   0.08500398  0.50863077\n",
      "   0.33595364  0.24516052 -0.89058928 -0.17632313  0.73806681  0.8507843\n",
      "   0.46431517 -0.74210604 -0.95554152  0.69693463 -0.73609391 -0.95954415\n",
      "   0.26750353  0.50710774 -0.93876063 -0.92911004  0.97131222 -0.08548193\n",
      "   0.44362522  0.93422654  0.63455436  0.83815472  0.11190417 -0.36560607\n",
      "   0.98590111  0.76277784 -0.44932585 -0.93868664  0.95476006  0.59041157\n",
      "  -0.44573259  0.7880957  -0.4624963  -0.51957411  0.63464155 -0.77580563\n",
      "  -0.6762849  -0.20641499  0.59702002 -0.99475925  0.69424812 -0.30368565\n",
      "  -0.97802807 -0.82250172 -0.8315502  -0.2804114 ]\n",
      " [-0.95385633 -0.78755295  0.78910324  0.99195198 -0.40589787 -0.97840237\n",
      "   0.98418432 -0.88127761  0.41520078 -0.47251492  0.97732125  0.99813004\n",
      "  -0.58306282 -0.89761811  0.89188714  0.90031802  0.99475939  0.06700888\n",
      "   0.69583283 -0.04051192  0.95318868 -0.90892582 -0.38188574  0.86277978\n",
      "  -0.95562296 -0.59278486  0.20679212 -0.5745836  -0.01101749 -0.31332572\n",
      "   0.48377927 -0.71295201 -0.00770488  0.99209471 -0.16656685  0.7664939\n",
      "  -0.98740023  0.47450716 -0.97879977 -0.26458654  0.94149466  0.41240979\n",
      "   0.95351578  0.16404919  0.53936188  0.82187583 -0.46635404 -0.60430459\n",
      "  -0.9289939  -0.41478563  0.86751037 -0.75431679 -0.63360452  0.97189003\n",
      "  -0.06801944  0.34295272  0.84688207 -0.92774178 -0.976308    0.68244236\n",
      "  -0.55096484 -0.65075006  0.65063553 -0.38754535  0.29788366 -0.3983518\n",
      "  -0.94973162  0.97125911 -0.95833633  0.50736568 -0.9679752  -0.00697451\n",
      "  -0.3644437  -0.17510231 -0.99839748 -0.00573612  0.9424437  -0.55191067\n",
      "   0.5599793   0.79867642 -0.6057409  -0.26647741  0.98757672  0.84902971\n",
      "  -0.80664847  0.7365248  -0.9163109  -0.60321455  0.05219512 -0.24873515\n",
      "   0.69128771 -0.45747868 -0.11286597  0.01312297 -0.38021543  0.22997108\n",
      "  -0.44648761 -0.77795143 -0.43592538 -0.93999457]\n",
      " [-0.31818014 -0.97543563  0.98404724  0.98696364 -0.99621672 -0.21287799\n",
      "   0.43849452 -0.06743215  0.4900202  -0.85953922 -0.94352944 -0.97254113\n",
      "  -0.96410096 -0.96187684  0.82791813  0.96898245  0.35691393 -0.9697694\n",
      "  -0.60698791  0.29384456  0.99389     0.94152873 -0.95046701  0.45792455\n",
      "   0.36712045  0.74877322  0.86002903 -0.54130375 -0.77985798  0.86456015\n",
      "  -0.05881542 -0.75156053 -0.59341099  0.96884025  0.98898012 -0.97734502\n",
      "  -0.92643304 -0.49454214 -0.99553363  0.67057252  0.83079945 -0.62946332\n",
      "   0.93558827  0.5438574  -0.478347   -0.42827092  0.45177482  0.74180945\n",
      "   0.86964403  0.28772032  0.98366896  0.25141188  0.90839057 -0.50839221\n",
      "  -0.82601029 -0.73247781  0.94955767  0.01487808  0.01849294 -0.89939654\n",
      "  -0.75990839 -0.35132157  0.79377516 -0.94496889 -0.64827062 -0.90004954\n",
      "  -0.72299014  0.82156046 -0.27983082 -0.98720695  0.15693157 -0.41805828\n",
      "  -0.82466017 -0.62027463  0.7487215   0.95191843 -0.64512227 -0.93236208\n",
      "   0.89785428  0.70610921  0.16764009  0.19827097  0.04021272  0.34727569\n",
      "   0.99338515  0.98876096 -0.97161599  0.83242026  0.27240064 -0.56202615\n",
      "   0.96238814  0.69238424 -0.97831573  0.38261422  0.22054258  0.99443064\n",
      "  -0.80115883 -0.54202846 -0.94469911  0.12352931]]\n",
      "Dataset (X):\n",
      "[[ 0.51459868 -0.05369564  1.55959884 ...  0.56548788  1.10956482\n",
      "   0.64754112]\n",
      " [-0.99962407  1.46620393 -1.10932716 ...  0.3163979   1.05531368\n",
      "   0.78429994]\n",
      " [ 0.18258542  0.51780768 -0.24688498 ... -1.01904053 -0.06018188\n",
      "  -1.05716552]\n",
      " ...\n",
      " [-0.29534408  1.39244791 -0.93568659 ... -0.26154742 -0.60317134\n",
      "   0.17969745]\n",
      " [ 0.39737484 -0.12905242 -0.15637631 ...  1.18319085 -1.27346938\n",
      "   1.54408932]\n",
      " [-0.6447867  -0.10099625 -0.58289847 ...  0.47114734 -1.82503895\n",
      "   1.25656632]]\n",
      "\n",
      "Weights (W):\n",
      "[[ 0.0703285  -0.09207616  0.00046712 ...  0.02878498  0.0114036\n",
      "   0.02639193]\n",
      " [ 0.05616378 -0.00774921 -0.01831405 ... -0.01155112  0.04080291\n",
      "  -0.06469014]\n",
      " [-0.11917771 -0.03050455 -0.12201427 ... -0.00902066 -0.0402458\n",
      "  -0.04474773]]\n",
      "\n",
      "Result (Y):\n",
      "[[-1.53217963 -1.18965792  0.41765409  1.67432202  0.99370468 -1.55805065\n",
      "   0.37785387  1.77745934  0.699951    0.24318296  3.93546067  1.2611915\n",
      "   1.62419509  0.7833101   0.06650761 -0.28537647  1.51216743 -0.17199809\n",
      "  -1.51771526 -1.40637181  1.04940595  0.49643467 -1.60202901 -1.57564246\n",
      "  -0.60150085 -1.01181009 -0.38433123 -0.1639282  -0.74936741  0.85492869\n",
      "   0.84833615 -0.73276578  0.67579529 -1.12297641  0.06987453 -0.1796488\n",
      "   1.51935651  0.91172295  1.58712129  1.11594894  0.33808039 -0.36588512\n",
      "   1.74632563 -0.97418987  0.65259753 -1.81592786  1.19433215 -1.09079028\n",
      "   0.26510383 -3.80888744  1.85389635 -2.91671781  0.08520961  0.56088095\n",
      "   0.34952434  0.25025731 -1.42476749 -0.17818528  0.94621965  1.25898594\n",
      "   0.50279848 -0.95515077 -1.89193334  0.86131512 -0.94189984 -1.9401279\n",
      "   0.274173    0.55882853 -1.72750699 -1.65184265  2.11499173 -0.08569106\n",
      "   0.47673531  1.69062318  0.74900397  1.21493833  0.11237482 -0.38334176\n",
      "   2.47386582  1.00282453 -0.48385527 -1.7268841   1.88302118  0.67829765\n",
      "  -0.4793621   1.06638587 -0.5004822  -0.5757562   0.74914995 -1.03474803\n",
      "  -0.82223572 -0.20942388  0.6885039  -2.97090694  0.8561102  -0.31357472\n",
      "  -2.25004503 -1.16450224 -1.19314011 -0.28812852]\n",
      " [-1.87290026 -1.06495506  1.06905054  2.75572189 -0.43069009 -2.25873061\n",
      "   2.41598078 -1.38145925  0.44187906 -0.51330323  2.23403489  3.48702562\n",
      "  -0.66709059 -1.45982245  1.43107721  1.47389582  2.9709205   0.06710945\n",
      "   0.8591759  -0.0405341   1.86554677 -1.52131082 -0.40226549  1.30411905\n",
      "  -1.89287091 -0.6819488   0.20981782 -0.65433884 -0.01101794 -0.32422893\n",
      "   0.52790661 -0.89316203 -0.00770503  2.76470463 -0.16813345  1.01177191\n",
      "  -2.530452    0.51587126 -2.26811681 -0.27103381  1.75104771  0.4385114\n",
      "   1.86913657  0.16554506  0.60325525  1.1625705  -0.50540087 -0.69990046\n",
      "  -1.650994   -0.44137754  1.32292853 -0.98289591 -0.74741545  2.12531167\n",
      "  -0.06812463  0.35743501  1.24502287 -1.64192925 -2.21192407  0.83367122\n",
      "  -0.61976565 -0.77659861  0.77639998 -0.40890831  0.30719558 -0.42168832\n",
      "  -1.82903537  2.1140534  -1.92511052  0.55917583 -2.05912489 -0.00697462\n",
      "  -0.38200071 -0.17692557 -3.56426057 -0.00573618  1.75946927 -0.62112477\n",
      "   0.63280303  1.09494644 -0.70216612 -0.27306812  2.53754951  1.25266662\n",
      "  -1.11735941  0.94284096 -1.56552426 -0.69818514  0.0522426  -0.2540641\n",
      "   0.85041787 -0.49411796 -0.11334892  0.01312373 -0.40031146  0.23415893\n",
      "  -0.48030471 -1.04016047 -0.46718912 -1.73800268]\n",
      " [-0.32962095 -2.1936237   2.4116311   2.51331044 -3.13420919 -0.21618403\n",
      "   0.47036541 -0.06753464  0.53608692 -1.29157783 -1.76927074 -2.13719429\n",
      "  -2.00103972 -1.97041746  1.18148132  2.07535956  0.37334483 -2.08840852\n",
      "  -0.70413823  0.30276898  2.89395811  1.75134773 -1.83659265  0.49468198\n",
      "   0.38509091  0.97015688  1.29345617 -0.60599785 -1.04500799  1.3111264\n",
      "  -0.05888338 -0.97653158 -0.6829147   2.07303641  2.59783834 -2.23456533\n",
      "  -1.63261467 -0.54205526 -3.05104549  0.81178271  1.19071165 -0.74052677\n",
      "   1.70143542  0.60961695 -0.52083861 -0.4577773   0.486928    0.95449094\n",
      "   1.33161722  0.29607905  2.3998178   0.25691938  1.51824067 -0.56055919\n",
      "  -1.17544627 -0.93405271  1.82726355  0.01487917  0.01849504 -1.46905242\n",
      "  -0.99599823 -0.36695062  1.08155533 -1.78255133 -0.77230992 -1.47248031\n",
      "  -0.9138817   1.16159948 -0.28749851 -2.52279176  0.15823923 -0.44533675\n",
      "  -1.17121154 -0.72545133  0.97003917  1.85183447 -0.76689835 -1.67616481\n",
      "   1.4610394   0.87938126  0.16923752  0.20093212  0.04023442  0.36234249\n",
      "   2.85413642  2.58793691 -2.12039149  1.19596681  0.27945505 -0.63578993\n",
      "   1.97729898  0.85252107 -2.25670699  0.40311864  0.22422635  2.94041719\n",
      "  -1.10183957 -0.60702348 -1.78003676  0.12416346]]\n"
     ]
    }
   ],
   "source": [
    "# Xavier Initialization Function\n",
    "def xavier_initialization(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initializes weights using Xavier initialization.\n",
    "    Args:\n",
    "        input_dim (int): Number of input features.\n",
    "        output_dim (int): Number of output features.\n",
    "    Returns:\n",
    "        np.ndarray: Initialized weights matrix.\n",
    "    \"\"\"\n",
    "    std = np.sqrt(2 / (input_dim + output_dim))\n",
    "    return np.random.normal(0, std, size = (output_dim, input_dim))\n",
    "\n",
    "# Initialize weights with Xavier initialization\n",
    "input_dim = X.shape[1]  # 2 (number of features)\n",
    "output_dim = 3          # Arbitrary, 3 output neurons\n",
    "W = xavier_initialization(input_dim, output_dim)\n",
    "\n",
    "# Compute Y = W @ X.T\n",
    "Y = W @ X.T\n",
    "Y_tanh = np.tanh(Y)\n",
    "\n",
    "# Output\n",
    "print(\"Dataset (X):\")\n",
    "print(X)\n",
    "print(\"\\nWeights (W):\")\n",
    "print(W)\n",
    "print(\"\\nResult (Y):\")\n",
    "print(Y)\n",
    "print(\"\\nResult after applying tanh (Y_tanh):\")\n",
    "print(Y_tanh)\n",
    "# Output\n",
    "print(\"Dataset (X):\")\n",
    "print(X)\n",
    "print(\"\\nWeights (W):\")\n",
    "print(W)\n",
    "print(\"\\nResult (Y):\")\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b7ae2d06-5ab9-44c6-b25b-fcdb693263f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.3684757858670006e-16,\n",
       " 1.3553527641383567,\n",
       " -0.005344922023346962,\n",
       " 0.7124418287348369)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.mean(),Y.std(),Y_tanh.mean(),Y_tanh.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5cf79547-6520-47fd-98a8-9a865457a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0007147019799603141, 0.005089017674485319)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.mean(),2*W.std()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "159b2d7f-dc61-40bb-a425-95f8c80ed193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00249115  0.09699207 -0.018032   ... -0.14983046 -0.14472735\n",
      "  -0.04509705]\n",
      " [ 0.04499795  0.04241577  0.08155452 ... -0.12840334 -0.06034991\n",
      "   0.0025271 ]\n",
      " [-0.02916729 -0.13338675  0.24451018 ...  0.09099708  0.05159747\n",
      "  -0.24490353]\n",
      " ...\n",
      " [-0.14753845  0.02598074  0.03149147 ... -0.03177622 -0.01333582\n",
      "  -0.04212599]\n",
      " [ 0.17140791 -0.27036593  0.08019689 ... -0.01189096  0.09493685\n",
      "   0.00241299]\n",
      " [-0.12766068 -0.00202925  0.02562901 ... -0.0325587  -0.11338704\n",
      "   0.08776577]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def glorot_normal(fan_in, fan_out, gain=1.0):\n",
    "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(0, std, size=(fan_in, fan_out))\n",
    "\n",
    "# Example usage\n",
    "fan_in = 128\n",
    "fan_out = 64\n",
    "weights = glorot_normal(fan_in, fan_out)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "931225ce-1a4d-4eed-82d3-7250b964427e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00028515810775374753, 1.330856200831036)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.mean(),128*weights.std()**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a7b8503-478a-4207-a258-a6c7e533869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187499388.97072685"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28 * 28*489.0372009277344**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "400ec748-0395-4d36-8908-bc232f1576ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.3140\n",
      "Accuracy: 90.71%\n",
      "\n",
      "Layer Statistics Analysis:\n",
      "Linear(in_features=256, out_features=10, bias=True)\n",
      "(tensor([[ 9.8999e-07,  1.6689e-08,  1.0875e-06,  1.4777e-07, -7.4708e-04,\n",
      "          3.3184e-07,  1.5955e-05,  9.1725e-05,  1.6933e-05,  6.1990e-04],\n",
      "        [ 7.0882e-09,  1.2783e-06,  4.6241e-05, -1.9321e-04,  7.1497e-07,\n",
      "          4.3170e-06,  2.7438e-07,  9.6850e-06,  1.2383e-04,  6.8701e-06],\n",
      "        [ 5.0292e-07,  5.4804e-05, -9.6169e-03,  5.9401e-04,  7.4253e-07,\n",
      "          5.8818e-06,  9.2242e-07,  1.5147e-03,  6.7079e-03,  7.3749e-04],\n",
      "        [ 2.1388e-06,  3.1482e-07,  2.5090e-07,  1.8435e-04,  2.4260e-08,\n",
      "         -2.4042e-04,  5.5982e-09,  3.9774e-07,  4.8862e-05,  4.0689e-06],\n",
      "        [-7.1893e-04,  1.5011e-07,  3.6959e-07,  1.8420e-05,  1.1493e-08,\n",
      "          6.6980e-04,  7.9445e-08,  1.4780e-08,  2.9882e-05,  1.9834e-07],\n",
      "        [ 6.8374e-08,  4.5444e-05,  2.0103e-05,  3.9389e-05,  6.8328e-04,\n",
      "          4.4831e-05,  2.9836e-05,  2.5608e-03,  7.2237e-05, -3.4960e-03],\n",
      "        [ 1.5771e-07,  1.2892e-07, -5.1847e-06,  1.7759e-07,  3.2059e-07,\n",
      "          2.3773e-08,  2.5300e-06,  1.7045e-09,  1.8000e-06,  4.5627e-08],\n",
      "        [ 2.9710e-07,  1.1608e-09,  8.3206e-09,  6.1608e-08, -2.6595e-05,\n",
      "          1.2457e-05,  1.7717e-06,  4.7398e-06,  2.1093e-06,  5.1503e-06],\n",
      "        [ 3.2409e-08,  9.7125e-07,  1.7036e-07,  4.6763e-04, -1.3812e-03,\n",
      "          5.0134e-05,  5.0082e-07,  1.1350e-04,  1.2344e-04,  6.2484e-04],\n",
      "        [ 1.1889e-07, -1.0347e-04,  5.3397e-05,  1.1797e-05,  1.5134e-07,\n",
      "          4.1548e-06,  1.9456e-06,  6.3758e-06,  2.4230e-05,  1.2986e-06],\n",
      "        [ 2.1268e-07,  3.6478e-07,  2.1557e-05,  5.4303e-07,  3.8829e-07,\n",
      "          1.1974e-05, -5.8414e-05,  4.4163e-10,  2.3341e-05,  3.2779e-08],\n",
      "        [ 5.2709e-06,  2.5702e-05,  1.3853e-05,  2.5961e-05,  1.0235e-05,\n",
      "          2.4554e-04,  1.7094e-05,  2.0784e-08, -3.4429e-04,  6.1885e-07],\n",
      "        [ 1.1891e-05,  3.3093e-06,  1.0040e-03,  3.0087e-06, -2.4277e-03,\n",
      "          2.2102e-04,  2.2466e-04,  2.6191e-04,  8.5386e-05,  6.1250e-04],\n",
      "        [ 8.9284e-07,  4.6244e-07, -2.1348e-04,  1.7156e-04,  3.4552e-09,\n",
      "          2.3461e-07,  3.7566e-08,  1.0410e-07,  4.0089e-05,  9.2475e-08],\n",
      "        [ 1.3580e-07, -7.4094e-05,  3.3077e-05,  9.2972e-06,  4.9639e-07,\n",
      "          1.1548e-06,  8.1220e-07,  1.1722e-05,  1.6506e-05,  8.9026e-07],\n",
      "        [ 3.8731e-07,  7.1741e-07,  5.1190e-07,  4.7832e-05, -4.7489e-04,\n",
      "          1.4092e-05,  4.6353e-07,  4.0855e-05,  5.9848e-06,  3.6405e-04],\n",
      "        [ 9.0957e-07,  7.9211e-07,  2.4285e-05,  1.2418e-05,  1.8979e-05,\n",
      "          2.3888e-06,  5.1949e-06,  1.0075e-04,  1.9482e-04, -3.6054e-04],\n",
      "        [ 2.5698e-08, -7.4835e-05,  1.5699e-05,  9.3179e-06,  5.8924e-08,\n",
      "          1.7545e-05,  4.6790e-06,  2.8384e-06,  2.2933e-05,  1.7372e-06],\n",
      "        [ 1.2301e-08,  1.1091e-06, -1.5388e-04,  4.7973e-06,  2.5242e-07,\n",
      "          2.9691e-06,  1.4313e-04,  1.0827e-07,  1.4215e-06,  7.7402e-08],\n",
      "        [ 6.6464e-09,  1.7830e-05,  7.8305e-05,  1.5683e-05,  5.4439e-06,\n",
      "          8.2786e-07,  2.9575e-07, -1.6099e-04,  2.7960e-07,  4.2322e-05],\n",
      "        [ 2.0347e-07,  2.5751e-07,  5.1480e-05, -7.8312e-05,  2.7932e-07,\n",
      "          2.1896e-06,  1.6345e-07,  5.0317e-08,  2.1370e-05,  2.3182e-06],\n",
      "        [ 1.9518e-06,  7.8024e-08,  4.0685e-05,  3.9178e-07,  2.5326e-06,\n",
      "          1.2844e-06, -5.7052e-05,  4.7513e-08,  9.1203e-06,  9.6082e-07],\n",
      "        [ 5.1973e-06,  2.7745e-07,  2.3187e-03, -2.4556e-03,  1.0279e-06,\n",
      "          2.9456e-05,  1.3541e-05,  2.2843e-07,  8.2043e-05,  5.1887e-06],\n",
      "        [ 2.0290e-07,  9.2516e-08,  8.0667e-06, -2.4193e-05,  1.3492e-09,\n",
      "          6.9202e-06,  4.5339e-10,  8.2686e-07,  3.1103e-06,  4.9715e-06],\n",
      "        [ 1.2320e-07,  4.3065e-06,  6.5487e-07,  2.8256e-05,  5.9718e-06,\n",
      "          8.8917e-06,  5.6758e-08,  4.5194e-03,  4.5886e-06, -4.5723e-03],\n",
      "        [ 2.3472e-08,  3.1106e-05,  7.7370e-07,  4.3529e-05,  7.9625e-04,\n",
      "          1.0481e-05,  3.8720e-07,  2.3591e-04,  2.6162e-05, -1.1446e-03],\n",
      "        [ 5.4513e-08,  1.2809e-07,  7.3608e-06,  3.1372e-08,  3.1142e-06,\n",
      "          6.5915e-07, -1.3685e-05,  2.2679e-08,  1.7392e-06,  5.7565e-07],\n",
      "        [ 3.6370e-05,  1.6426e-06,  1.7619e-04,  1.4605e-04,  1.2048e-04,\n",
      "          1.5486e-03,  1.1022e-04,  1.7659e-06, -2.5498e-03,  4.0840e-04],\n",
      "        [ 1.3788e-08,  4.4428e-05,  3.6270e-05,  6.5419e-06, -1.6292e-03,\n",
      "          5.3462e-06,  7.3733e-05,  3.9782e-05,  2.1386e-05,  1.4017e-03],\n",
      "        [-1.2601e-06,  3.0218e-10,  3.4777e-07,  3.6030e-08,  1.1299e-10,\n",
      "          1.2127e-07,  9.6469e-09,  9.8210e-09,  6.9582e-07,  3.9438e-08],\n",
      "        [ 6.8084e-06,  2.7063e-05,  3.7881e-04,  3.4034e-04,  1.7710e-06,\n",
      "          1.1631e-04,  6.6773e-05,  2.1855e-07, -9.4889e-04,  1.0798e-05],\n",
      "        [ 3.4414e-08,  1.2426e-06,  7.8914e-06,  6.7929e-06,  3.0233e-04,\n",
      "          8.3976e-07,  1.2975e-06,  1.7078e-04,  6.2920e-06, -4.9749e-04],\n",
      "        [-6.9480e-04,  1.8745e-07,  3.8540e-04,  5.9667e-06,  7.6437e-09,\n",
      "          2.7609e-04,  3.6949e-06,  1.0451e-06,  2.2289e-05,  1.2315e-07],\n",
      "        [ 8.5904e-07,  3.4230e-09,  1.2353e-06,  8.1271e-07,  5.5966e-05,\n",
      "          2.7444e-07,  1.6145e-07,  1.3190e-05,  6.0938e-06, -7.8596e-05],\n",
      "        [ 5.4888e-08,  3.8644e-08,  6.1804e-07, -6.4960e-06,  1.7215e-07,\n",
      "          3.7502e-06,  2.9932e-09,  1.0174e-07,  5.1289e-07,  1.2457e-06],\n",
      "        [ 4.1246e-08,  1.3199e-08,  3.2561e-07,  1.0869e-08, -9.8133e-06,\n",
      "          1.5932e-07,  6.8157e-06,  4.3592e-07,  4.1151e-07,  1.6009e-06],\n",
      "        [ 4.3426e-08,  2.0922e-06,  1.0627e-04,  3.0811e-06,  1.1011e-05,\n",
      "          1.7440e-05, -1.4550e-04,  1.9642e-08,  5.2907e-06,  2.4978e-07],\n",
      "        [ 3.8756e-05,  4.3508e-08,  1.1417e-06,  6.0807e-04,  6.6719e-06,\n",
      "         -1.2092e-03,  7.8925e-07,  1.2722e-06,  6.3668e-05,  4.8880e-04],\n",
      "        [ 2.6948e-05,  6.2442e-08, -2.9166e-04,  2.8106e-05,  1.8295e-07,\n",
      "          6.9550e-07,  4.2263e-06,  2.0399e-05,  2.0504e-04,  5.9972e-06],\n",
      "        [ 1.6628e-06,  1.8550e-08,  3.3910e-05,  1.4417e-04,  1.1844e-06,\n",
      "         -8.6836e-04,  2.8791e-07,  8.5549e-08,  6.7200e-04,  1.5041e-05],\n",
      "        [ 6.4947e-07,  1.4221e-07,  8.7688e-06,  9.3825e-07,  2.4566e-05,\n",
      "          9.6220e-07,  2.9493e-07,  3.4174e-05,  1.0933e-04, -1.7982e-04],\n",
      "        [ 4.2363e-07,  6.7181e-06,  5.2942e-07,  1.4136e-05,  2.8156e-03,\n",
      "          2.7001e-04,  1.7089e-06,  4.7711e-05,  4.5617e-06, -3.1614e-03],\n",
      "        [ 1.9949e-06,  1.1658e-08,  5.5300e-09,  1.5343e-06,  2.2042e-07,\n",
      "          1.7641e-06,  3.6087e-08, -1.8661e-05,  1.2707e-06,  1.1824e-05],\n",
      "        [ 4.7466e-09,  3.7246e-08,  1.4701e-07,  4.3984e-09, -1.2043e-05,\n",
      "          2.6590e-07,  2.0026e-06,  5.7797e-07,  2.1971e-06,  6.8060e-06],\n",
      "        [ 4.9435e-05,  5.0433e-09,  8.3788e-07,  2.0654e-05,  8.4799e-06,\n",
      "          2.7441e-06,  1.8066e-06,  2.5641e-04,  1.4970e-04, -4.9007e-04],\n",
      "        [ 1.1367e-04,  1.6155e-04,  2.5345e-04,  3.8763e-04, -3.1837e-03,\n",
      "          9.3500e-05,  1.6906e-03,  3.8189e-05,  2.6428e-05,  4.1866e-04],\n",
      "        [ 9.0572e-08,  1.9474e-07,  7.5775e-07,  7.5258e-06,  1.0965e-06,\n",
      "          8.9967e-07,  8.5687e-08,  2.3317e-08, -1.7009e-05,  6.3347e-06],\n",
      "        [ 1.1171e-04,  1.7054e-06,  2.4485e-05, -3.1450e-04,  7.9921e-08,\n",
      "          1.3816e-04,  1.4731e-07,  7.3667e-07,  3.6653e-05,  8.1500e-07],\n",
      "        [ 4.5861e-06,  1.1728e-07, -1.8362e-04,  2.3377e-06,  3.3308e-06,\n",
      "          2.8841e-07,  5.3597e-06,  3.1175e-07,  1.2366e-04,  4.3628e-05],\n",
      "        [ 1.0400e-09, -4.9403e-05,  1.0689e-06,  2.5063e-05,  3.4614e-07,\n",
      "          6.4928e-07,  3.0369e-07,  5.8418e-06,  6.9863e-06,  9.1415e-06],\n",
      "        [ 6.2666e-07,  4.4050e-09,  8.3681e-08,  2.2102e-05,  2.3931e-05,\n",
      "          9.2795e-07,  1.6610e-08,  2.5410e-04,  1.9135e-06, -3.0370e-04],\n",
      "        [ 4.1418e-07,  1.7224e-05,  1.8676e-06, -1.2851e-03,  1.9639e-06,\n",
      "          6.2594e-04,  1.4894e-06,  8.6917e-06,  2.0674e-05,  6.0687e-04],\n",
      "        [-3.0253e-04,  1.3101e-07,  4.6630e-05,  1.2272e-04,  1.0144e-06,\n",
      "          4.9164e-05,  2.7537e-06,  2.4503e-05,  1.2883e-05,  4.2734e-05],\n",
      "        [ 7.5084e-09,  4.6219e-06,  1.8439e-07,  1.9421e-05,  2.6525e-04,\n",
      "          3.7792e-06,  1.4394e-07,  3.3867e-04,  4.7011e-06, -6.3678e-04],\n",
      "        [ 2.4965e-05,  1.4861e-05,  1.3914e-05,  3.0063e-04,  4.0798e-07,\n",
      "         -6.2179e-04,  1.9035e-05,  3.1755e-05,  3.5932e-05,  1.8029e-04],\n",
      "        [ 3.0352e-09,  6.9023e-08,  6.8024e-08,  5.7202e-07, -1.0154e-04,\n",
      "          7.4495e-07,  1.5574e-07,  2.5937e-06,  3.7823e-06,  9.3556e-05],\n",
      "        [-8.6095e-05,  4.7801e-08,  2.1756e-05,  1.6839e-06,  1.2087e-09,\n",
      "          5.1760e-05,  1.9253e-06,  6.7265e-08,  8.7260e-06,  1.2862e-07],\n",
      "        [ 5.6446e-06,  9.1852e-05,  1.1887e-05, -3.0427e-04,  9.7691e-08,\n",
      "          1.6306e-04,  2.4036e-05,  5.9904e-07,  6.4664e-06,  6.2762e-07],\n",
      "        [ 1.5496e-05,  2.9698e-06,  4.1098e-05, -1.9070e-04,  4.6488e-09,\n",
      "          1.1243e-04,  4.0133e-09,  3.2425e-07,  1.8018e-05,  3.5363e-07],\n",
      "        [ 3.7678e-06,  1.2558e-08,  2.5944e-07,  1.4002e-06,  5.3899e-07,\n",
      "          4.5730e-06,  3.9870e-09, -4.2921e-04,  5.6824e-07,  4.1809e-04],\n",
      "        [ 1.3176e-06,  1.2308e-08,  4.0955e-07,  1.1229e-05,  2.5214e-07,\n",
      "          9.1970e-06,  5.3516e-09, -1.2778e-04,  2.1205e-06,  1.0324e-04],\n",
      "        [ 6.1881e-09,  1.7248e-05,  1.2855e-05,  3.0151e-06,  7.4895e-07,\n",
      "          2.0535e-07,  2.3937e-07, -1.5196e-04,  1.7218e-05,  1.0043e-04],\n",
      "        [ 3.5377e-07,  1.2491e-06,  4.5700e-07, -3.6959e-05,  3.4370e-08,\n",
      "          3.2852e-05,  6.8700e-08,  2.1326e-07,  2.1541e-07,  1.5143e-06],\n",
      "        [ 3.0663e-07,  1.3010e-07,  2.0431e-06,  2.6314e-06, -2.8324e-05,\n",
      "          1.6889e-06,  1.0212e-05,  1.8584e-06,  7.9536e-07,  8.6586e-06],\n",
      "        [ 6.9030e-07,  6.4784e-07,  2.8493e-07,  5.2109e-04,  1.9952e-05,\n",
      "          1.2347e-05,  1.8592e-08,  4.4725e-04,  4.7788e-05, -1.0501e-03],\n",
      "        [ 3.3050e-09, -7.7616e-05,  9.0842e-06,  4.7365e-06,  5.4739e-08,\n",
      "          6.7000e-06,  2.1696e-06,  4.8531e-07,  5.3935e-05,  4.4916e-07],\n",
      "        [ 6.2315e-09, -5.8091e-05,  2.5256e-05,  1.0615e-05,  1.2146e-07,\n",
      "          1.0108e-06,  2.1807e-06,  7.4889e-06,  9.9310e-06,  1.4817e-06],\n",
      "        [ 8.4272e-08,  8.2413e-07,  1.1398e-06,  2.8427e-06,  1.1206e-06,\n",
      "          7.3924e-07,  1.0235e-08, -2.1492e-04,  5.1813e-07,  2.0764e-04],\n",
      "        [ 1.0842e-04,  4.2559e-09,  1.2233e-05,  8.8071e-08,  7.9537e-04,\n",
      "          2.5519e-05,  2.1032e-05,  9.2449e-05,  1.7474e-05, -1.0726e-03],\n",
      "        [ 5.5200e-07,  1.3415e-06,  1.0265e-06,  1.8054e-06,  2.2786e-05,\n",
      "          2.1049e-05,  1.7716e-06,  6.4052e-08, -6.9102e-05,  1.8707e-05],\n",
      "        [ 1.0262e-06, -5.9380e-04,  1.9675e-04,  9.0530e-05,  1.0485e-06,\n",
      "          1.6082e-04,  1.2439e-05,  1.3655e-05,  1.1083e-04,  6.7118e-06],\n",
      "        [ 4.4442e-07,  9.2986e-08,  6.0597e-06, -1.8003e-05,  4.8214e-09,\n",
      "          1.0025e-05,  7.6883e-09,  4.5772e-09,  1.2849e-06,  7.9078e-08],\n",
      "        [ 9.8508e-09,  6.8136e-07, -3.7202e-05,  7.8656e-07,  4.3592e-08,\n",
      "          2.5510e-06,  2.9012e-05,  1.3765e-08,  4.0707e-06,  3.0557e-08],\n",
      "        [ 1.0549e-06,  2.8160e-07,  1.3484e-04,  6.6240e-07,  3.6737e-06,\n",
      "          9.6328e-06, -1.5646e-04,  2.7931e-09,  6.2806e-06,  2.6973e-08],\n",
      "        [ 2.1158e-05,  7.7691e-07,  3.0620e-04, -6.6481e-04,  2.1811e-08,\n",
      "          6.5741e-05,  3.5489e-09,  5.0885e-05,  1.9153e-04,  2.8491e-05],\n",
      "        [ 3.1225e-08,  1.8847e-07,  2.4208e-06,  1.3460e-05,  3.5003e-08,\n",
      "          2.4108e-07,  1.2952e-09, -4.7823e-05,  1.2728e-06,  3.0172e-05],\n",
      "        [ 7.8590e-07,  3.8624e-09, -4.3381e-06,  1.5090e-06,  3.1257e-08,\n",
      "          3.0739e-08,  1.3291e-06,  1.4054e-09,  6.4167e-07,  4.5209e-09],\n",
      "        [ 1.2393e-05,  6.7912e-06,  1.0507e-03,  3.1442e-04,  1.5851e-05,\n",
      "          1.2848e-03,  5.0390e-06,  1.7785e-07, -2.6935e-03,  3.3617e-06],\n",
      "        [ 1.8458e-07,  1.5354e-08,  2.9739e-06,  7.3646e-08, -5.4415e-05,\n",
      "          1.2318e-06,  7.4411e-06,  1.0086e-05,  2.6624e-06,  2.9746e-05],\n",
      "        [-3.2513e-03,  2.2325e-07,  6.8454e-05,  1.1893e-03,  2.0172e-08,\n",
      "          1.2692e-03,  8.5861e-09,  5.7470e-05,  3.7239e-04,  2.9417e-04],\n",
      "        [ 3.0741e-05,  1.4041e-07,  2.0949e-06,  1.0463e-03,  1.4498e-05,\n",
      "         -2.0372e-03,  9.6633e-06,  6.0646e-06,  4.2473e-04,  5.0296e-04],\n",
      "        [-6.5502e-05,  5.5979e-08,  4.7695e-06,  3.1600e-05,  1.8301e-07,\n",
      "          3.3458e-06,  2.9446e-06,  2.0987e-07,  2.0139e-05,  2.2574e-06],\n",
      "        [ 2.9682e-06,  1.9543e-07,  3.2771e-05,  2.2723e-06, -2.0963e-03,\n",
      "          9.5296e-05,  8.7135e-04,  1.4914e-05,  2.9773e-05,  1.0468e-03],\n",
      "        [-3.2699e-04,  1.1796e-08,  1.0818e-05,  1.5503e-05,  4.9931e-08,\n",
      "          1.3151e-04,  1.2572e-07,  1.5735e-05,  1.9875e-05,  1.3336e-04],\n",
      "        [ 9.1545e-07,  7.2139e-07,  4.1070e-05,  1.3142e-07,  7.6335e-06,\n",
      "          3.4036e-05, -8.6945e-05,  1.5915e-07,  2.0524e-06,  2.2657e-07],\n",
      "        [ 1.5098e-06,  3.5969e-06,  1.0488e-04,  3.2541e-06, -1.1342e-03,\n",
      "          1.0769e-05,  1.6940e-04,  5.4128e-05,  2.1205e-05,  7.6542e-04],\n",
      "        [ 1.2865e-08, -1.3036e-04,  1.3238e-05,  3.8753e-05,  1.0187e-06,\n",
      "          4.8874e-06,  2.2371e-06,  4.9636e-05,  9.6736e-06,  1.0906e-05],\n",
      "        [ 3.4058e-06,  1.3118e-07, -1.1206e-04,  1.0014e-04,  2.5716e-07,\n",
      "          3.4037e-07,  2.0028e-06,  2.8862e-06,  2.2259e-06,  6.7176e-07],\n",
      "        [ 1.7801e-04,  6.0772e-08, -2.3462e-04,  3.7119e-07,  1.7954e-06,\n",
      "          3.1394e-05,  4.0637e-06,  3.7067e-07,  1.2076e-05,  6.4828e-06],\n",
      "        [ 3.1999e-08,  7.8568e-09,  1.3060e-06,  1.6087e-06,  3.5417e-09,\n",
      "          5.8492e-09,  6.2571e-11, -4.6073e-06,  2.8007e-07,  1.3634e-06],\n",
      "        [ 3.3810e-05,  1.5996e-07,  8.5420e-05,  3.9531e-06,  9.7917e-08,\n",
      "         -2.2292e-04,  3.7209e-06,  4.2321e-08,  9.5351e-05,  3.6761e-07],\n",
      "        [ 6.4199e-04,  2.1773e-06,  1.0538e-03,  3.0535e-05, -9.8177e-03,\n",
      "          1.0102e-03,  6.6709e-03,  1.1428e-04,  2.7703e-04,  1.6774e-05],\n",
      "        [ 2.6845e-08,  1.1001e-07,  7.7869e-07,  5.0015e-08, -1.8590e-05,\n",
      "          6.0851e-08,  3.3094e-06,  6.7658e-06,  1.9340e-06,  5.5552e-06],\n",
      "        [ 1.0559e-05,  1.1335e-08, -2.7680e-05,  9.7582e-06,  6.5892e-08,\n",
      "          8.6249e-08,  5.5658e-06,  9.4493e-08,  1.4890e-06,  5.0242e-08],\n",
      "        [ 4.8545e-07,  1.8144e-07,  4.1244e-05, -4.3835e-05,  1.2510e-09,\n",
      "          1.1879e-06,  1.5914e-09,  3.7198e-07,  2.3247e-07,  1.2701e-07],\n",
      "        [ 6.4983e-09,  5.2108e-08,  1.7818e-06,  1.3996e-08,  3.3388e-06,\n",
      "          1.3942e-07, -6.3749e-06,  8.1940e-09,  8.4168e-07,  1.9204e-07]]),)\n",
      "1.1156468110451456e-11\n",
      "4.899307555206178e-07\n",
      "-0.009817744605243206\n",
      "0.006707874592393637\n",
      "_______\n",
      "Tanh()\n",
      "(tensor([[-5.1878e-05,  5.6394e-05, -1.0779e-04,  ..., -4.5378e-05,\n",
      "          1.8874e-04, -1.0349e-05],\n",
      "        [-1.0587e-05,  4.6771e-05, -3.7257e-05,  ...,  3.4524e-05,\n",
      "         -1.1786e-05,  1.1049e-05],\n",
      "        [-8.2031e-04,  4.2188e-04, -1.7363e-03,  ...,  9.3593e-04,\n",
      "         -2.8468e-04, -5.7235e-04],\n",
      "        ...,\n",
      "        [-3.7087e-06, -1.6622e-06, -4.3378e-06,  ..., -3.8845e-07,\n",
      "         -1.0819e-06, -2.2322e-07],\n",
      "        [-6.6764e-08,  7.9431e-06, -2.5871e-06,  ...,  4.0424e-06,\n",
      "         -7.2821e-07,  4.2199e-06],\n",
      "        [ 1.2873e-06, -8.2208e-07,  7.3068e-07,  ..., -1.7815e-09,\n",
      "         -1.0874e-07, -2.4601e-07]]),)\n",
      "-2.0558379674184835e-06\n",
      "5.1906489773045905e-08\n",
      "-0.003275463590398431\n",
      "0.0031042732298374176\n",
      "_______\n",
      "Linear(in_features=784, out_features=256, bias=True)\n",
      "(tensor([[-2.9802e-05,  1.3673e-05, -2.2772e-05,  ..., -5.9434e-06,\n",
      "          1.5976e-05, -1.0087e-05],\n",
      "        [-3.0670e-08,  3.1527e-07, -2.1452e-09,  ...,  1.1170e-06,\n",
      "         -5.6909e-06,  3.4296e-07],\n",
      "        [-4.5757e-04,  4.1328e-04, -1.4494e-04,  ...,  7.2688e-04,\n",
      "         -1.0960e-04, -2.2816e-05],\n",
      "        ...,\n",
      "        [-1.4069e-07, -1.6069e-06, -9.0560e-09,  ..., -1.8985e-09,\n",
      "         -2.2499e-07, -1.7393e-07],\n",
      "        [-1.5510e-10,  2.3390e-08, -1.7653e-08,  ...,  7.4171e-07,\n",
      "         -4.4480e-07,  1.6347e-06],\n",
      "        [ 1.8544e-08, -7.6561e-08,  1.9285e-10,  ..., -1.5765e-10,\n",
      "         -6.4342e-09, -7.4946e-09]]),)\n",
      "-7.898893841229437e-07\n",
      "1.4817466365002474e-08\n",
      "-0.0026715972926467657\n",
      "0.002213451312854886\n",
      "_______\n",
      "\n",
      "input layer:\n",
      "mean: 0.023541\n",
      "var: 1.056724\n",
      "min: -0.424213\n",
      "max: 2.821487\n",
      "\n",
      "Linear(in_features=784, out_features=256, bias=True) layer:\n",
      "mean: -0.020882\n",
      "var: 3.327581\n",
      "min: -7.172306\n",
      "max: 7.887413\n",
      "\n",
      "fc1 layer:\n",
      "mean: -0.020882\n",
      "var: 3.327581\n",
      "min: -7.172306\n",
      "max: 7.887413\n",
      "\n",
      "Tanh() layer:\n",
      "mean: -0.000647\n",
      "var: 0.603367\n",
      "min: -0.999999\n",
      "max: 1.000000\n",
      "\n",
      "fc2 layer:\n",
      "mean: -0.000647\n",
      "var: 0.603367\n",
      "min: -0.999999\n",
      "max: 1.000000\n",
      "\n",
      "Linear(in_features=256, out_features=10, bias=True) layer:\n",
      "mean: 0.034110\n",
      "var: 13.351840\n",
      "min: -8.661047\n",
      "max: 11.805573\n",
      "\n",
      "fc3 layer:\n",
      "mean: 0.034110\n",
      "var: 13.351840\n",
      "min: -8.661047\n",
      "max: 11.805573\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch.nn.init as init\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Data preprocessing and loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Define the Simple Neural Network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "        # Xavier initialization\n",
    "        init.xavier_uniform_(self.fc1.weight, gain=1)\n",
    "        init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "        # To store activations and gradients\n",
    "        self.activations = {}\n",
    "        self.gradients = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        self.activations['input'] = x  # Store activations\n",
    "        x = self.fc1(x)\n",
    "        self.activations['fc1'] = x\n",
    "        x = self.fc2(x)\n",
    "        self.activations['fc2'] = x\n",
    "        x = self.fc3(x)\n",
    "        self.activations['fc3'] = x\n",
    "        return x\n",
    "\n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register hooks for activation and gradient tracking\"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations[module] = output\n",
    "\n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            self.gradients[module] = grad_output\n",
    "\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            layer.register_forward_hook(forward_hook)\n",
    "            layer.register_backward_hook(backward_hook)\n",
    "\n",
    "# Function to analyze activations at each layer\n",
    "def analyze_layer_statistics(model, images):\n",
    "    model(images)  # Forward pass\n",
    "    stats = {}\n",
    "    for layer_name, activations in model.activations.items():\n",
    "        stats[layer_name] = {\n",
    "            'mean': torch.mean(activations).item(),\n",
    "            'var': torch.var(activations).item(),\n",
    "            'min': torch.min(activations).item(),\n",
    "            'max': torch.max(activations).item()\n",
    "        }\n",
    "\n",
    "    for layer_name, activations in model.gradients.items():\n",
    "        print(layer_name)\n",
    "        print(activations)\n",
    "        print(torch.mean(activations[0]).item())\n",
    "        print(torch.var(activations[0]).item())\n",
    "        print(torch.min(activations[0]).item())\n",
    "        print(torch.max(activations[0]).item())\n",
    "        #print(activations.shape)\n",
    "        print('_______')\n",
    "    return stats\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = SimpleNN()\n",
    "model.register_hooks()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        output = model(images)  # Forward pass\n",
    "        loss = criterion(output, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "        correct += (torch.argmax(output, dim=1) == labels).sum().item()\n",
    "    \n",
    "    # Epoch statistics\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "    print(f\"Accuracy: {100 * correct / total:.2f}%\")\n",
    "    losses.append(running_loss / len(trainloader))\n",
    "    \n",
    "    # Analyze and display layer statistics\n",
    "    print(\"\\nLayer Statistics Analysis:\")\n",
    "    stats = analyze_layer_statistics(model, images)\n",
    "    for layer_name, layer_stats in stats.items():\n",
    "        print(f\"\\n{layer_name} layer:\")\n",
    "        for stat_name, value in layer_stats.items():\n",
    "            print(f\"{stat_name}: {value:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "    break\n",
    "# Plot loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc64b14f-d8e8-4f8b-a083-08a6e5d24c69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

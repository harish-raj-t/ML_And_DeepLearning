{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb860ad2-8886-4ff9-85a7-1797b6f61853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "file_path = \"input.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393e22a-b150-43a2-b076-eca605bd6cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt','r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22a4b5-dc23-4ff1-81ef-01a8c5e95388",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {char:idx for idx, char in enumerate(chars)}\n",
    "itos = {idx:char for idx, char in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda s: [itos[i] for i in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5ad437-edc4-4527-805a-66ae21b6af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BLOCK_SIZE = 8\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class AutoRegressiveDataset(Dataset):\n",
    "  def __init__(self,data, block_size):\n",
    "    self.data = data\n",
    "    self.block_size = block_size\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)-self.block_size\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    X= self.data[idx:idx+self.block_size]\n",
    "    y= self.data[idx+1:idx+self.block_size+1]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97503d2-57fd-4672-8743-d645d81e3495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AutoRegressiveDataset(data,BLOCK_SIZE)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07dfa0-a242-4aae-b45b-44def3a39e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "  def __init__(self, emd_dim, heads=4, dropout = 0.2):\n",
    "    super(MaskedMultiHeadAttention, self).__init__()\n",
    "    assert emd_dim % heads == 0\n",
    "    self.heads = heads\n",
    "    self.head_dim = emd_dim//heads\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "    self.multiHead = nn.Linear(emd_dim, emd_dim*3)\n",
    "    self.output = nn.Linear(emd_dim,emd_dim)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    qkv = self.multiHead(x)\n",
    "    q, k, v = torch.chunk(qkv,3,dim=-1)\n",
    "    q = q.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    k = k.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    v = v.view(B, T, self.heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "    tril = torch.tril(torch.ones(T,T))\n",
    "    attn_scores = attn_scores.masked_fill(tril==0, float('-inf'))\n",
    "    attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    attn_probs_drop = self.dropout(attn_probs)\n",
    "    attn_output = torch.matmul(attn_probs_drop,v)\n",
    "    fn_attn_output = attn_output.permute(0, 2, 1, 3).reshape(B, T, C)\n",
    "    return self.output(fn_attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb542655-a62e-467c-97f6-ac10ad43bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1D(nn.Module):\n",
    "  def __init__(self, dim, eps=1e-5):\n",
    "    super(LayerNorm1D, self).__init__()\n",
    "    self.gamma = nn.Parameter(torch.ones(dim))\n",
    "    self.beta = nn.Parameter(torch.zeros(dim))\n",
    "    self.eps = eps\n",
    "\n",
    "  def forward(self, x):\n",
    "    mean = x.mean(-1,keepdim=True)\n",
    "    var = x.var(-1, unbiased=False, keepdim=True)\n",
    "    xhat = (x-mean)/torch.sqrt(var+self.eps)\n",
    "    return (self.gamma * xhat) +self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c6a7b7-9237-4271-bec2-3900160e394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim, dropout = 0.2):\n",
    "    super().__init__()\n",
    "    self.feed_forward_layer = nn.Sequential(\n",
    "      nn.Linear(input_dim, hidden_dim),\n",
    "      nn.GeLU(),\n",
    "      nn.Linear(hidden_dim, output_dim),\n",
    "      nn.Dropout(dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.feed_forward_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5438c-c152-45ad-8054-9c0dac8ee146",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "  def __init__(self,embed_dim,heads=4):\n",
    "    super().__init__()\n",
    "    self.layer_norm1 = LayerNorm1D(embed_dim)\n",
    "    self.layer_norm2 = LayerNorm1D(embed_dim)\n",
    "    self.masked_multi_head_attn =  MaskedMultiHeadAttention(embed_dim, heads = 4)\n",
    "    self.feed_forward_layer = FeedForward(embed_dim, embed_dim*4, embed_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x + self.masked_multi_head_attn(self.layer_norm1(x))\n",
    "    x = x + self.feed_forward_layer(self.layer_norm2(x))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94590bb-2fee-42fd-8a35-6a140cb00256",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRegressiveModel(nn.Module):\n",
    "  def __init__(self, embed_dim, vocab_size, block_size = BLOCK_SIZE, heads=4, num_layers=4):\n",
    "    super().__init__()\n",
    "    self.block = nn.Sequential(*[Block(embed_dim,heads) for _ in range(num_layers)])\n",
    "    self.positional_embedding = nn.Embedding(block_size, embed_dim)\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.final_layer_norm = LayerNorm1D(embed_dim)\n",
    "    self.final_layer = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "  def forward(self, x, targets = None):\n",
    "    _, T = x.shape\n",
    "    x_emb = self.embedding(x)\n",
    "    x_pos_emb = self.positional_embedding(torch.arange(T))\n",
    "    x = x_emb + x_pos_emb\n",
    "    block_output = self.block(x)\n",
    "    x_out = self.final_layer_norm(block_output)\n",
    "    return self.final_layer(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d33f1c-8e6b-41fa-83ef-11751abd7c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoRegressiveModel(embed_dim=128, vocab_size=vocab_size, block_size= BLOCK_SIZE, heads = 4)\n",
    "if os.path.exists(\"decoder_transformers_autoregressive_model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"decoder_transformers_autoregressive_model.pth\")) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba3563-adc2-4d87-97f5-9590e034eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim, criterion: nn.Module, dataloader: DataLoader, epochs: int):\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for X,y in dataloader:\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "    print(f\"Epoch: {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d590b3-8de3-4950-8fca-efb7a2bde54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model: nn.Module,dataloader: DataLoader):\n",
    "  model.eval()\n",
    "  val_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for X,y in dataloader:\n",
    "      outputs = model(X)\n",
    "      B, T, _ = outputs.shape\n",
    "      loss = criterion(outputs.reshape(B*T,-1),y.reshape(B*T))\n",
    "      val_loss += loss.item()\n",
    "    print(f\"Loss: {val_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca5acb9-f0d7-4acc-ba66-7b46ca013322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: nn.Module, start_seq: str =\"The\", epochs = 100):\n",
    "  current = start_seq\n",
    "  content = [c for c in start_seq]\n",
    "  for _ in range(epochs):\n",
    "    value = torch.tensor(encode(current[-BLOCK_SIZE:])).unsqueeze(0)\n",
    "    outputs = model(value).squeeze(0)\n",
    "    probs = torch.softmax(outputs[-1], dim=-1)\n",
    "    indices = torch.multinomial(probs,1).tolist()\n",
    "    output = decode(indices)\n",
    "    content.append(output[0])\n",
    "    current = current + output[0]\n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb918c-a53c-46f8-83cb-827957ca7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion, train_loader, 10)\n",
    "val(model,val_loader)\n",
    "content = generate(model, epochs = 10000)\n",
    "''.join(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
